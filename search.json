[
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Machine Learning Explainability",
    "section": "",
    "text": "See the sidebar for an index of slides and demos."
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#expert-systems-1970s-1980s",
    "href": "slides/1_introduction/preliminaries.html#expert-systems-1970s-1980s",
    "title": "XML Preliminaries",
    "section": "Expert Systems (1970s & 1980s)",
    "text": "Expert Systems (1970s & 1980s)"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#transparent-machine-learning-models",
    "href": "slides/1_introduction/preliminaries.html#transparent-machine-learning-models",
    "title": "XML Preliminaries",
    "section": "Transparent Machine Learning Models",
    "text": "Transparent Machine Learning Models"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#rise-of-the-dark-side-deep-neural-networks",
    "href": "slides/1_introduction/preliminaries.html#rise-of-the-dark-side-deep-neural-networks",
    "title": "XML Preliminaries",
    "section": "Rise of the Dark Side (Deep Neural Networks)",
    "text": "Rise of the Dark Side (Deep Neural Networks)\n\n\n\n\n\n\n\n\n\nNo need to engineer features (by hand)\nHigh predictive power\nBlack-box modelling"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#darpas-xai-concept",
    "href": "slides/1_introduction/preliminaries.html#darpas-xai-concept",
    "title": "XML Preliminaries",
    "section": "DARPA‚Äôs XAI Concept",
    "text": "DARPA‚Äôs XAI Concept\n\n\n\n\n\n\n\n\nhttps://www.darpa.mil/program/explainable-artificial-intelligence"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#expectations-mismatch",
    "href": "slides/1_introduction/preliminaries.html#expectations-mismatch",
    "title": "XML Preliminaries",
    "section": "Expectations Mismatch",
    "text": "Expectations Mismatch\n\nWe tend to request explanations mostly when a phenomenon disagrees with our expectations.\nFor example, an ML model behaves unlike we envisaged and outputs an unexpected prediction."
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#stakeholders",
    "href": "slides/1_introduction/preliminaries.html#stakeholders",
    "title": "XML Preliminaries",
    "section": "Stakeholders",
    "text": "Stakeholders\n\n\n\n\n\n\n\n(Belle and Papantonis 2021)"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#purpose-or-role",
    "href": "slides/1_introduction/preliminaries.html#purpose-or-role",
    "title": "XML Preliminaries",
    "section": "Purpose or Role",
    "text": "Purpose or Role\n\n\n\nFairness\nPrivacy\nReliability and Robustness\nCausality\nTrust\n\n\n\nTrustworthiness / Reliability / Robustness / Causality\n\nNo silly mistakes & socially acceptable\n\nFairness\n\nDoes not discriminate & is not biased\n\n\n\n\n\n\n\nPrivacy ‚Äì no data leakage\nReliability and Robustness ‚Äì no undesired predictive behaviour, e.g., large changes for small changes in input\nCausality ‚Äì model only relies on true causal relations, and not spurious correlations\n\n\n\n(Doshi-Velez and Kim 2017)"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#benefits",
    "href": "slides/1_introduction/preliminaries.html#benefits",
    "title": "XML Preliminaries",
    "section": "Benefits",
    "text": "Benefits\n\n\n\nNew knowledge\n\nAids in scientific discovery\n\nLegislation\n\nDoes not break the law\n\n\nEU‚Äôs General Data Protection Regulation\nCalifornia Consumer Privacy Act\n\n\n\n\nDebugging / Auditing\n\nIdentify modelling errors and mistakes\n\nHuman‚ÄìAI co-operation\n\nHelp humans complete tasks\n\n\n\n\n\n\ndebugging and auditing ‚Äì husky vs.¬†wolf in presence of background snow; (we will see later why this conclusion may have been incorrect)"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#drawbacks",
    "href": "slides/1_introduction/preliminaries.html#drawbacks",
    "title": "XML Preliminaries",
    "section": "Drawbacks",
    "text": "Drawbacks\n\nSafety / Security\n\nAbuse transparency to steal a (proprietary) model\n\nManipulation\n\nUse transparency to game a system, e.g., credit scoring"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#pitfalls",
    "href": "slides/1_introduction/preliminaries.html#pitfalls",
    "title": "XML Preliminaries",
    "section": "Pitfalls",
    "text": "Pitfalls\n\n\n\n\n\n\n\n\n\n\n\n\nThe danger of ‚Äúempty explanations‚Äù ‚Äì statements that look like explanations (‚Ä¶because‚Ä¶) but carry no meaningful explanation\nWhen asking for a favor, providing a reason increases the chances of success\n\n\n‚ÄúExcuse me, I have 5 pages. May I use the Xerox machine?‚Äù (request only)\n‚ÄúExcuse me, I have 5 pages. May I use the xerox machine, because I‚Äôm in a rush?‚Äù (request with a real reason)\n‚ÄúExcuse me, I have 5 pages. May I use the xerox machine, because I have to make copies?‚Äù (request with a fake reason)\n\n\n\n\n(Langer, Blank, and Chanowitz 1978)"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#explainability-source",
    "href": "slides/1_introduction/preliminaries.html#explainability-source",
    "title": "XML Preliminaries",
    "section": "Explainability Source",
    "text": "Explainability Source\n\nante-hoc ‚Äì intrinsically transparent predictive models (transparency by design)\npost-hoc ‚Äì derived from a pre-existing predictive models that may themselves be unintelligible (usually requires an additional explanatory modelling step)\n\n\n\nThese techniques can be model-specific or model-agnostic (based on input‚Äìoutput paris / observations)"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#explanation-provenance",
    "href": "slides/1_introduction/preliminaries.html#explanation-provenance",
    "title": "XML Preliminaries",
    "section": "Explanation Provenance",
    "text": "Explanation Provenance\n\n ¬†¬†¬† Ante-hoc does not necessarily entail explainable or human-understandable\n\n\nendogenous explanation ‚Äì based on human-comprehensible concepts operated on by a transparent model\nexogenous explanation ‚Äì based on human-comprehensible concepts constructed outside of the predictive model (by the additional modelling step)\n\n\n\nInformation lineage ‚Äì the sources of explanatory information\nIn the definition module, we will see why ante-hoc transparency may not entail explainability / interpretability (human understanding)\nJust a brief note: We may preserve all the desired properties of ante-hoc interpretability (intrinsic transparency of the model) but still derive explanations not directly from its transparency\nFor example, process a complex decision tree ‚Äì transparent by design ‚Äì to extract a counterfactual from adjacent leaves, thus making it explainable"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#explanation-types-1",
    "href": "slides/1_introduction/preliminaries.html#explanation-types-1",
    "title": "XML Preliminaries",
    "section": "Explanation Types",
    "text": "Explanation Types\n\nmodel-based ‚Äì derived from model internals\nfeature-based ‚Äì regarding importance or influence of data features\ninstance-based ‚Äì carried by rael or fictitious data point\n\n\n\n\nmeta-explainers ‚Äì one of the above, but not extracted directly from the predictive model being explained (using an additional explainability modelling step, e.g., surrogate)\n\n\n\nThe categories are not clear cut; a linear model explained through its coefficients is both model-based and feature-based\nThese insights can be communicated via different media (as we will see later)\n\nnumerical summarisation ‚Äì a single number or their collection (e.g., a table)\nvisualisation\ntextualisation"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#explainability-scope",
    "href": "slides/1_introduction/preliminaries.html#explainability-scope",
    "title": "XML Preliminaries",
    "section": "Explainability Scope",
    "text": "Explainability Scope\n\n\n\n\n\n\n\n\n\n\nglobal\ncohort\nlocal\n\n\n\n\ndata\na set of data\na subset of data\nan instance\n\n\nmodel\nmodel space\nmodel subspace\na point in model space\n\n\nprediction\na set of predictions\na subset of predictions\na individual prediction\n\n\n\n\n\n\n\nalgorithmic explanation ‚Äì the learning algorithm, not the resulting model; e.g., modelling assumptions, caveats, compatible data types, etc.\n\n\n\nlocal explanations do not generalise to other instances\nwhile data points may appear similar, our perception of similarity is likely to be different from the similarity the underlying model is using\na collection of cohort explanations may be easier to comprehend than a single global ‚Äì recall the difference between transparency and explainability\nthis requires a suitable explanation aggregation policy that humans can execute"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#explainability-target",
    "href": "slides/1_introduction/preliminaries.html#explainability-target",
    "title": "XML Preliminaries",
    "section": "Explainability Target",
    "text": "Explainability Target\n\nFocused on a single class (technically limited)\n\nimplicit context\n\nWhy \\(A\\)? (‚Ä¶and not anything else, i.e., \\(B \\cup C \\cup \\ldots\\))\n\nexplicit context\n\nWhy \\(A\\) and not \\(B\\)?\n\n\nMulti-class explainability (Sokol and Flach 2020b)\n\nIf üåßÔ∏è, then \\(A\\); else if ‚òÄÔ∏è & ü•∂, then \\(B\\), else ‚òÄÔ∏è & ü•µ, then \\(C\\)."
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#where-is-the-human-circa-2017",
    "href": "slides/1_introduction/preliminaries.html#where-is-the-human-circa-2017",
    "title": "XML Preliminaries",
    "section": "Where Is the Human? (circa 2017)",
    "text": "Where Is the Human? (circa 2017)\n \n\n\n(Miller 2019)"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#exploding-complexity-2019",
    "href": "slides/1_introduction/preliminaries.html#exploding-complexity-2019",
    "title": "XML Preliminaries",
    "section": "Exploding Complexity (2019)",
    "text": "Exploding Complexity (2019)\n \n\n\n\nHigh-stakes vs.¬†low-stakes\nAnte-hoc vs.¬†post-hoc\n\n\n\n(Rudin 2019)"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#xai-process",
    "href": "slides/1_introduction/preliminaries.html#xai-process",
    "title": "XML Preliminaries",
    "section": "XAI process",
    "text": "XAI process\n A generic eXplainable Artificial Intelligence process is beyond our reach at the moment\n\nXAI Taxonomy spanning social and technical desiderata:\n‚Ä¢ Functional ‚Ä¢ Operational ‚Ä¢ Usability ‚Ä¢ Safety ‚Ä¢ Validation ‚Ä¢\n(Sokol and Flach, 2020. Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches)\nFramework for black-box explainers\n(Henin and Le M√©tayer, 2019. Towards a generic framework for black-box explanations of algorithmic decision systems)"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#books",
    "href": "slides/1_introduction/preliminaries.html#books",
    "title": "XML Preliminaries",
    "section": "üìñ ¬† Books",
    "text": "üìñ ¬† Books\n\nSurvey of machine learning interpretability in form of an online book\nOverview of explanatory model analysis published as an online book"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#papers",
    "href": "slides/1_introduction/preliminaries.html#papers",
    "title": "XML Preliminaries",
    "section": "üìù ¬† Papers",
    "text": "üìù ¬† Papers\n\nGeneral introduction to interpretability (Sokol and Flach 2021)\nIntroduction to human-centred explainability (Miller 2019)\nCritique of post-hoc explainability (Rudin 2019)\nSurvey of interpretability techniques (Guidotti et al. 2018)\nTaxonomy of explainability approaches (Sokol and Flach 2020a)"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#software",
    "href": "slides/1_introduction/preliminaries.html#software",
    "title": "XML Preliminaries",
    "section": "üíΩ ¬† Software",
    "text": "üíΩ ¬† Software\n\n\n\nMicrosoft‚Äôs Interpret\nOracle‚Äôs Skater\nIBM‚Äôs Explainability 360\nFAT Forensics\nDALEX\n\n\n\nalibi\niml\nLIME (Python, R)\nSHAP (Python, R)"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#summary",
    "href": "slides/1_introduction/preliminaries.html#summary",
    "title": "XML Preliminaries",
    "section": "Summary",
    "text": "Summary\n\nThe landscape of explainability is fast-paced and complex\nDon‚Äôt expect universal solution\nThe involvement of humans ‚Äì as explainees ‚Äì makes it all the more complicated"
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#bibliography",
    "href": "slides/1_introduction/preliminaries.html#bibliography",
    "title": "XML Preliminaries",
    "section": "Bibliography",
    "text": "Bibliography\n\n\nBelle, Vaishak, and Ioannis Papantonis. 2021. ‚ÄúPrinciples and Practice of Explainable Machine Learning.‚Äù Frontiers in Big Data, 39.\n\n\nDoshi-Velez, Finale, and Been Kim. 2017. ‚ÄúTowards a Rigorous Science of Interpretable Machine Learning.‚Äù arXiv Preprint arXiv:1702.08608.\n\n\nGuidotti, Riccardo, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. ‚ÄúA Survey of Methods for Explaining Black Box Models.‚Äù ACM Computing Surveys (CSUR) 51 (5): 1‚Äì42.\n\n\nLanger, Ellen J, Arthur Blank, and Benzion Chanowitz. 1978. ‚ÄúThe Mindlessness of Ostensibly Thoughtful Action: The Role of ‚ÄòPlacebic‚Äô Information in Interpersonal Interaction.‚Äù Journal of Personality and Social Psychology 36 (6): 635.\n\n\nMiller, Tim. 2019. ‚ÄúExplanation in Artificial Intelligence: Insights from the Social Sciences.‚Äù Artificial Intelligence 267: 1‚Äì38.\n\n\nRudin, Cynthia. 2019. ‚ÄúStop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.‚Äù Nature Machine Intelligence 1 (5): 206‚Äì15.\n\n\nSokol, Kacper, and Peter Flach. 2020a. ‚ÄúExplainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches.‚Äù In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 56‚Äì67.\n\n\n‚Äî‚Äî‚Äî. 2020b. ‚ÄúLIMEtree: Interactively Customisable Explanations Based on Local Surrogate Multi-Output Regression Trees.‚Äù arXiv Preprint arXiv:2005.01427.\n\n\n‚Äî‚Äî‚Äî. 2021. ‚ÄúExplainability Is in the Mind of the Beholder: Establishing the Foundations of Explainable Artificial Intelligence.‚Äù arXiv Preprint arXiv:2112.14466."
  },
  {
    "objectID": "slides/1_introduction/preliminaries.html#questions",
    "href": "slides/1_introduction/preliminaries.html#questions",
    "title": "XML Preliminaries",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\nkacper.sokol@rmit.edu.au  k.sokol@bristol.ac.uk"
  },
  {
    "objectID": "slides/1_introduction/intro.html#running-example-counterfactual-explanations",
    "href": "slides/1_introduction/intro.html#running-example-counterfactual-explanations",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Running Example: Counterfactual Explanations",
    "text": "Running Example: Counterfactual Explanations\n\n\n\nHad you been 10 years younger, your loan application would be accepted."
  },
  {
    "objectID": "slides/1_introduction/intro.html#f-functional-requirements",
    "href": "slides/1_introduction/intro.html#f-functional-requirements",
    "title": "Introduction to Machine Learning Explainability",
    "section": "(F) Functional Requirements",
    "text": "(F) Functional Requirements\n\n\n\nF1 Problem Supervision Level\nF2 Problem Type\nF3 Explanation Target\nF4 Explanation Breadth/Scope\nF5 Computational Complexity\n\n\n\nF6 Applicable Model Class\nF7 Relation to the Predictive System\nF8 Compatible Feature Types\nF9 Caveats and Assumptions"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section",
    "href": "slides/1_introduction/intro.html#section",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "F1 Problem Supervision Level\n\n\n\nunsupervised\nsemi-supervised\nsupervised\nreinforcement\n\n\n\n\n\nF2 Problem Type\n\n\n\nclassification\n\nprobabilistic / non-probabilistic\nbinary / multi-class\nmulti-label\n\nregression\nclustering"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-1",
    "href": "slides/1_introduction/intro.html#section-1",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "F6 Applicable Model Class\n\n\n\nmodel-agnostic\nmodel class-specific\nmodel-specific\n\n\n\n\n\nF7 Relation to the Predictive System\n\n\n\nante-hoc (based on endogenous information)\npost-hoc (based on exogenous information)"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-2",
    "href": "slides/1_introduction/intro.html#section-2",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "F5 Computational Complexity\n\n\n\noff-line explanations\nreal-time explanations\n\n\n\n\n\nF8 Compatible Feature Types\n\n\n\nnumerical\ncategorical (one-hot encoding)\n\n\n\n\n\nF9 Caveats and Assumptions\n\n\n\nany underlying assumptions, e.g., black box linearity"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-3",
    "href": "slides/1_introduction/intro.html#section-3",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "F3 Explanation Target\n\n\n\ndata (both raw data and features)\nmodels\npredictions\n\n\n\n\n\nF4 Explanation Breadth/Scope\n\n\n\nlocal ‚Äì data point / prediction\ncohort ‚Äì subgroup / subspace\nglobal"
  },
  {
    "objectID": "slides/1_introduction/intro.html#u-usability-requirements",
    "href": "slides/1_introduction/intro.html#u-usability-requirements",
    "title": "Introduction to Machine Learning Explainability",
    "section": "(U) Usability Requirements",
    "text": "(U) Usability Requirements\n\n\n\nU1 Soundness\nU2 Completeness\nU3 Contextfullness\nU4 Interactiveness\nU5 Actionability\nU6 Chronology\n\n\n\nU7 Coherence\nU8 Novelty\nU9 Complexity\nU10 Personalisation\nU11 Parsimony"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-4",
    "href": "slides/1_introduction/intro.html#section-4",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "U1 Soundness\n\n\nHow truthful it is with respect to the black box?\n\n\n(‚úî)\n\n\n\n\nU2 Completeness\n\n\nHow well does it generalise?\n\n\n(‚úó)\n\n\n\n\nU3 Contextfullness\n\n\n‚ÄúIt only holds for people older than 25.‚Äù\n\n\n\n\n\n\nU11 Parsimony\n\n\nHow short is it?\n\n\n(‚úî)"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-5",
    "href": "slides/1_introduction/intro.html#section-5",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "U6 Chronology\n\n\nMore recent events first.\n\n\n\n\nU7 Coherence\n\n\nComply with the natural laws (mental model).\n\n\n\n\nU8 Novelty\n\n\nAvoid stating obvious / being a truism.\n\n\n\n\n\n\nU9 Complexity\n\n\nAppropriate for the audience."
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-6",
    "href": "slides/1_introduction/intro.html#section-6",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "U5 Actionability\n\n\nActionable foil.\n\n\n(‚úî)\n\n\n\n\nU4 Interactiveness\n\n\nUser-defined foil.\n\n\n(‚úî)\n\n\n\n\nU10 Personalisation\n\n\nUser-defined foil.\n\n\n(‚úî)"
  },
  {
    "objectID": "slides/1_introduction/intro.html#o-operational-requirements",
    "href": "slides/1_introduction/intro.html#o-operational-requirements",
    "title": "Introduction to Machine Learning Explainability",
    "section": "(O) Operational Requirements",
    "text": "(O) Operational Requirements\n\n\n\nO1 Explanation Family\nO2 Explanatory Medium\nO3 System Interaction\nO4 Explanation Domain\nO5 Data and Model Transparency\n\n\n\nO6 Explanation Audience\nO7 Function of the Explanation\nO8 Causality vs.¬†Actionability\nO9 Trust vs.¬†Performance\nO10 Provenance"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-7",
    "href": "slides/1_introduction/intro.html#section-7",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "O1 Explanation Family\n\n\n\nassociations between antecedent and consequent\ncontrasts and differences\ncausal mechanisms\n\n\n\n\n\nO2 Explanatory Medium\n\n\n\n(statistical / numerical) summarisation\nvisualisation\ntextualisation\nformal argumentation\n\n\n\n\n\nO3 System Interaction\n\n\n\nstatic ‚Äì one-directional\ninteractive ‚Äì bi-directional"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-8",
    "href": "slides/1_introduction/intro.html#section-8",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "O4 Explanation Domain\n\n\n\noriginal domain (exemplars, model parameters)\ntransformed domain (interpretable representation)\n\n\n\n\n\nO5 Data and Model Transparency\n\n\n\ntransparent/opaque data\ntransparent/opaque model\n\n\n\n\n\nO6 Explanation Audience\n\n\n\ndomain experts\nlay audience"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-9",
    "href": "slides/1_introduction/intro.html#section-9",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "O7 Function of the Explanation\n\n\n\ninterpretability\nfairness (disparate impact)\naccountability (model robustness / adversarial examples)\n\n\n\n\n\nO8 Causality vs.¬†Actionability\n\n\n\nlook like causal insights but aren‚Äôt\n\n\n\n\n\nO9 Trust and Performance\n\n\n\ntruthful to the black-box (perfect fidelity)\npredictive performance is not affected"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-10",
    "href": "slides/1_introduction/intro.html#section-10",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "O10 Provenance\n\n\n\npredictive model\ndata set\npredictive model and data set (explainability trace)"
  },
  {
    "objectID": "slides/1_introduction/intro.html#s-safety-requirements",
    "href": "slides/1_introduction/intro.html#s-safety-requirements",
    "title": "Introduction to Machine Learning Explainability",
    "section": "(S) Safety Requirements",
    "text": "(S) Safety Requirements\n\nS1 Information Leakage\nS2 Explanation Misuse\nS3 Explanation Invariance\nS4 Explanation Quality"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-11",
    "href": "slides/1_introduction/intro.html#section-11",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "S1 Information Leakage\n\n\nContrastive explanation leak precise values.\n\n\n\n\nS2 Explanation Misuse\n\n\nCan be used to reverse-engineer the black box.\n\n\n\n\nS3 Explanation Invariance\n\n\nDoes it always output the same explanation (stochasticity / stability)?\n\n\n\n\nS4 Explanation Quality\n\n\nIs it from the data distribution?  How far from a decision boundary (confidence)?"
  },
  {
    "objectID": "slides/1_introduction/intro.html#v-validation-requirements",
    "href": "slides/1_introduction/intro.html#v-validation-requirements",
    "title": "Introduction to Machine Learning Explainability",
    "section": "(V) Validation Requirements",
    "text": "(V) Validation Requirements\n\nV1 User Studies\nV2 Synthetic Experiments"
  },
  {
    "objectID": "slides/1_introduction/intro.html#section-12",
    "href": "slides/1_introduction/intro.html#section-12",
    "title": "Introduction to Machine Learning Explainability",
    "section": "",
    "text": "V1 User Studies\n\n\n\nTechnical correctness\nHuman biases\nUnfounded generalisation\nUsefulness\n\n\n\n\n\nV2 Synthetic Experiments"
  },
  {
    "objectID": "slides/1_introduction/intro.html#examples",
    "href": "slides/1_introduction/intro.html#examples",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "slides/1_introduction/intro.html#lime-explainability-fact-sheet",
    "href": "slides/1_introduction/intro.html#lime-explainability-fact-sheet",
    "title": "Introduction to Machine Learning Explainability",
    "section": "LIME Explainability Fact Sheet",
    "text": "LIME Explainability Fact Sheet"
  },
  {
    "objectID": "slides/1_introduction/intro.html#challenges",
    "href": "slides/1_introduction/intro.html#challenges",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Challenges",
    "text": "Challenges\n\nThe desiderata list is neither exhaustive nor prescriptive\nSome properties are incompatible or competing ‚Äì choose wisely and justify your choices\n\nShould I focus more on property F42 or F44?\nFor O13, should I go for X or Y?\n\nOther properties cannot be answered uniquely\n\nE.g., coherence with the user‚Äôs mental model\n\nThe taxonomy does not define explainability"
  },
  {
    "objectID": "slides/1_introduction/intro.html#lack-of-a-universally-accepted-definition",
    "href": "slides/1_introduction/intro.html#lack-of-a-universally-accepted-definition",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Lack of a universally accepted definition",
    "text": "Lack of a universally accepted definition\n\n\nSimulatability\n(Lipton, 2018. The mythos of model interpretability)\nThe Chinese Room Theorem\n(Searle, 1980. Minds, brains, and programs)\nMental Models\n(Kulesza et al., 2013. Too much, too little, or just right? Ways explanations impact end users‚Äô mental models)\n\nFunctional ‚Äì operationalisation without understanding\nStructural ‚Äì appreciation of the underlying mechanism"
  },
  {
    "objectID": "slides/1_introduction/intro.html#defining-explainability",
    "href": "slides/1_introduction/intro.html#defining-explainability",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Defining explainability",
    "text": "Defining explainability\n\\[\n\\texttt{Explainability} \\; =\n\\] \\[\n\\underbrace{ \\texttt{Reasoning} \\left( \\texttt{Transparency} \\; | \\; \\texttt{Background Knowledge} \\right)}_{\\textit{understanding}}\n\\]\n\nTransparency ‚Äì insight (of arbitrary complexity) into operation of a system\nBackground Knowledge ‚Äì implicit or explicit exogenous information\nReasoning ‚Äì algorithmic or mental processing of information\n\n\n\nSokol and Flach, 2021. Explainability Is in the Mind of the Beholder: Establishing the Foundations of Explainable Artificial Intelligence"
  },
  {
    "objectID": "slides/1_introduction/intro.html#understanding-explainability-transparency",
    "href": "slides/1_introduction/intro.html#understanding-explainability-transparency",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Understanding, explainability & transparency",
    "text": "Understanding, explainability & transparency\n\n\nA continuous spectrum rather than a binary property"
  },
  {
    "objectID": "slides/1_introduction/intro.html#automated-decision-making",
    "href": "slides/1_introduction/intro.html#automated-decision-making",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Automated Decision-making",
    "text": "Automated Decision-making"
  },
  {
    "objectID": "slides/1_introduction/intro.html#na√Øve-view",
    "href": "slides/1_introduction/intro.html#na√Øve-view",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Na√Øve view",
    "text": "Na√Øve view"
  },
  {
    "objectID": "slides/1_introduction/intro.html#evaluation-tiers",
    "href": "slides/1_introduction/intro.html#evaluation-tiers",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Evaluation Tiers",
    "text": "Evaluation Tiers\n\n\n\n\nHumans\nTask\n\n\n\n\nApplication-grounded Evaluation\nReal Humans\nReal Tasks\n\n\nHuman-grounded Evaluation\nReal Humans\nSimple Tasks\n\n\nFunctionally-grounded Evaluation\nNo Real Humans\nProxy Tasks\n\n\n\n\n\nKim and Doshi-Velez, 2017. Towards A Rigorous Science of Interpretable Machine Learning"
  },
  {
    "objectID": "slides/1_introduction/intro.html#explanatory-insight-presentation-medium",
    "href": "slides/1_introduction/intro.html#explanatory-insight-presentation-medium",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Explanatory insight & presentation medium",
    "text": "Explanatory insight & presentation medium"
  },
  {
    "objectID": "slides/1_introduction/intro.html#phenomenon-explanation",
    "href": "slides/1_introduction/intro.html#phenomenon-explanation",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Phenomenon & explanation",
    "text": "Phenomenon & explanation"
  },
  {
    "objectID": "slides/1_introduction/intro_deux.html#associations-between-antecedent-and-consequent",
    "href": "slides/1_introduction/intro_deux.html#associations-between-antecedent-and-consequent",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Associations Between Antecedent and Consequent",
    "text": "Associations Between Antecedent and Consequent\n\n\n\nfeature importance\nfeature attribution / influence\nrules\n\n\n\nexemplars (prototypes & criticisms)"
  },
  {
    "objectID": "slides/1_introduction/intro_deux.html#contrasts-and-differences",
    "href": "slides/1_introduction/intro_deux.html#contrasts-and-differences",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Contrasts and Differences",
    "text": "Contrasts and Differences\n\n(non-causal) counterfactuals\ni.e., contrastive statements\nprototypes & criticisms"
  },
  {
    "objectID": "slides/1_introduction/intro_deux.html#causal-mechanisms",
    "href": "slides/1_introduction/intro_deux.html#causal-mechanisms",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Causal Mechanisms",
    "text": "Causal Mechanisms\n\ncausal counterfactuals\ncausal chains\nfull causal model"
  },
  {
    "objectID": "slides/1_introduction/intro_deux.html#permutation-feature-importance",
    "href": "slides/1_introduction/intro_deux.html#permutation-feature-importance",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Permutation Feature Importance",
    "text": "Permutation Feature Importance\n\n\n\n\n\n\n\nhttps://www.kaggle.com/code/dansbecker/permutation-importance"
  },
  {
    "objectID": "slides/1_introduction/intro_deux.html#individual-conditional-expectation-partial-dependence",
    "href": "slides/1_introduction/intro_deux.html#individual-conditional-expectation-partial-dependence",
    "title": "Introduction to Machine Learning Explainability",
    "section": "Individual Conditional Expectation &  Partial Dependence",
    "text": "Individual Conditional Expectation &  Partial Dependence"
  },
  {
    "objectID": "slides/1_introduction/intro_deux.html#face-counterfactuals",
    "href": "slides/1_introduction/intro_deux.html#face-counterfactuals",
    "title": "Introduction to Machine Learning Explainability",
    "section": "FACE Counterfactuals",
    "text": "FACE Counterfactuals\n\n\n\n\n\n\n\nPoyiadzi, Sokol, Santos-Rodriguez, De Bie and Flach, 2020. FACE: Feasible and actionable counterfactual explanations"
  },
  {
    "objectID": "slides/1_introduction/intro_deux.html#rulefit",
    "href": "slides/1_introduction/intro_deux.html#rulefit",
    "title": "Introduction to Machine Learning Explainability",
    "section": "RuleFit",
    "text": "RuleFit\n\n\n\n\n\n\n\nhttps://christophm.github.io/interpretable-ml-book/rulefit.html"
  },
  {
    "objectID": "slides/1_introduction/course.html#about-me",
    "href": "slides/1_introduction/course.html#about-me",
    "title": "Machine Learning Explainability",
    "section": "About Me",
    "text": "About Me\n\nEducation\n\nMaster of Engineering in Mathematics and Computer Science (University of Bristol)\nDoctor of Philosophy in Computer Science ‚Äì Artificial Intelligence and Machine Learning (University of Bristol)"
  },
  {
    "objectID": "slides/1_introduction/course.html#about-me-meta-subs.ctd",
    "href": "slides/1_introduction/course.html#about-me-meta-subs.ctd",
    "title": "Machine Learning Explainability",
    "section": "About Me ¬†¬†¬†",
    "text": "About Me ¬†¬†¬†\n\nWork Experience\n\nResearch Fellow, ARC Centre of Excellence for Automated Decision-Making and Society (RMIT University)\nHonorary Research Fellow, Intelligent Systems Laboratory (University of Bristol)"
  },
  {
    "objectID": "slides/1_introduction/course.html#about-me-meta-subs.ctd-1",
    "href": "slides/1_introduction/course.html#about-me-meta-subs.ctd-1",
    "title": "Machine Learning Explainability",
    "section": "About Me ¬†¬†¬†",
    "text": "About Me ¬†¬†¬†\n\nWork Experience ¬†¬†¬†\n\nSenior Research Associate, TAILOR: European Union‚Äôs AI Research Excellence Centre (University of Bristol)\nResearch Associate, Human-Centred Artificial Intelligence in collaboration with THALES (University of Bristol)\nResearch Assistant, REFrAMe (University of Bristol)"
  },
  {
    "objectID": "slides/1_introduction/course.html#about-me-meta-subs.ctd-2",
    "href": "slides/1_introduction/course.html#about-me-meta-subs.ctd-2",
    "title": "Machine Learning Explainability",
    "section": "About Me ¬†¬†¬†",
    "text": "About Me ¬†¬†¬†\n\nWork Experience ¬†¬†¬†\n\nVisiting Research Fellow, AI and Humanity Summer Cluster, Simons Institute for the Theory of Computing (UC Berkeley)\nVisiting Researcher, Ubiquitous Computing Research Group (Universit√† della Svizzera italiana)\nVisiting Researcher, Machine Learning research group (University of Tartu)"
  },
  {
    "objectID": "slides/1_introduction/course.html#about-me-meta-subs.ctd-3",
    "href": "slides/1_introduction/course.html#about-me-meta-subs.ctd-3",
    "title": "Machine Learning Explainability",
    "section": "About Me ¬†¬†¬†",
    "text": "About Me ¬†¬†¬†\n\nResearch\n\nAI & ML transparency, interpretability and explainability\nTaxonomy of XAI & IML\nHuman-centred, interactive and personalised XAI & IML"
  },
  {
    "objectID": "slides/1_introduction/course.html#about-me-meta-subs.ctd-4",
    "href": "slides/1_introduction/course.html#about-me-meta-subs.ctd-4",
    "title": "Machine Learning Explainability",
    "section": "About Me ¬†¬†¬†",
    "text": "About Me ¬†¬†¬†\n\nResearch ¬†¬†¬†\n\nRobustness, accountability, modularity and parameterisation of XAI & IML\nPost-hoc methods, e.g., surrogates"
  },
  {
    "objectID": "slides/1_introduction/course.html#about-me-meta-subs.ctd-5",
    "href": "slides/1_introduction/course.html#about-me-meta-subs.ctd-5",
    "title": "Machine Learning Explainability",
    "section": "About Me ¬†¬†¬†",
    "text": "About Me ¬†¬†¬†\n\nResearch ¬†¬†¬†\n\nXAI & IML open-source software\nXAI & IML freely available, online, interactive training materials"
  },
  {
    "objectID": "slides/1_introduction/course.html#about-you",
    "href": "slides/1_introduction/course.html#about-you",
    "title": "Machine Learning Explainability",
    "section": "About You",
    "text": "About You\n\nWho are you?\nWhat‚Äôs your background?\nWhy are you taking this course?\nWhat do you expect to get out of this course?\n\n\n\nRound of introductions"
  },
  {
    "objectID": "slides/1_introduction/course.html#schedule",
    "href": "slides/1_introduction/course.html#schedule",
    "title": "Machine Learning Explainability",
    "section": "Schedule",
    "text": "Schedule\n\nWhen: 2 weeks ‚Äì w/c 6th and 13th of February 2023\nWhat: 10 sessions comprising of\n\n45-minute lecture\n45-minute supervised lab session (project / assignment / coursework)\n45-minute open office (general questions & project discussions)\n\nTime Commitment: 20 hours (self-study)\n\n\n\nWe may use the last session for project presentations"
  },
  {
    "objectID": "slides/1_introduction/course.html#schedule-meta-subs.ctd",
    "href": "slides/1_introduction/course.html#schedule-meta-subs.ctd",
    "title": "Machine Learning Explainability",
    "section": "Schedule ¬†¬†¬†",
    "text": "Schedule ¬†¬†¬†\n\nWhere:\n\n\n\n\n\n\n\n\nWhat\nTime\nLocation\n\n\n\n\nlecture\nTBC\nTBC\n\n\nlab\nTBC\nTBC\n\n\nopen office\nTBC\nTBC"
  },
  {
    "objectID": "slides/1_introduction/course.html#prerequisites",
    "href": "slides/1_introduction/course.html#prerequisites",
    "title": "Machine Learning Explainability",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nPython programming\nBasic mathematical concepts (relevant to machine learning)\nMachine learning techniques for tabular data\n‚≠ê Prior experience with machine learning approaches for images and text (e.g., deep learning) or other forms of data modelling (e.g., time series forecasting, reinforcement learning) if you decide to pursue a project in this direction"
  },
  {
    "objectID": "slides/1_introduction/course.html#resources",
    "href": "slides/1_introduction/course.html#resources",
    "title": "Machine Learning Explainability",
    "section": "Resources",
    "text": "Resources\n\n\nOnline Slides\n\nusi.xmlx.io\nxmlx-io.github.io/usi-slides-mirror\n\nGitHub Repository\n\ngithub.com/xmlx-io/usi-slides-source (raw slides source)\ngithub.com/xmlx-io/usi-slides (html slides source)\ngithub.com/xmlx-io/usi-slides-mirror (html slides source mirror)"
  },
  {
    "objectID": "slides/1_introduction/course.html#resources-meta-subs.ctd",
    "href": "slides/1_introduction/course.html#resources-meta-subs.ctd",
    "title": "Machine Learning Explainability",
    "section": "Resources ¬†¬†¬†",
    "text": "Resources ¬†¬†¬†\n\nSlides\n\nWritten in Markdown\nBuilt with Quarto into reveal.js (web-based slides)\nComputational elements and figures coded in Python (with matplotlib)\nSource can be compiled into Jupyter Notebooks (to experiment, modify, adapt and reuse the code chunks)\n\n\n\nIf you struggle to convert these, let me know and I will help you out"
  },
  {
    "objectID": "slides/1_introduction/course.html#motivation-fa-minus-square",
    "href": "slides/1_introduction/course.html#motivation-fa-minus-square",
    "title": "Machine Learning Explainability",
    "section": "Motivation ¬†¬†¬†",
    "text": "Motivation ¬†¬†¬†\n\n\nWealth of XAI and IML learning resources‚Ä¶\n‚Ä¶but mostly limited to\n\nsummary descriptions\ncode examples\nexplanation examples\ninterpretation tips"
  },
  {
    "objectID": "slides/1_introduction/course.html#motivation-fa-plus-square",
    "href": "slides/1_introduction/course.html#motivation-fa-plus-square",
    "title": "Machine Learning Explainability",
    "section": "Motivation ¬†¬†¬†",
    "text": "Motivation ¬†¬†¬†\n\n\nDeconstruct each method\nInspect its assumptions and operationalisation\nLearn to tune explainers for the problem at hand\nLearn to interpret explanations in view of their theoretical properties and (limitations of) algorithmic implementation\n\n\n\nDevelop critical thinking about XAI and IML techniques"
  },
  {
    "objectID": "slides/1_introduction/course.html#general-learning-objectives",
    "href": "slides/1_introduction/course.html#general-learning-objectives",
    "title": "Machine Learning Explainability",
    "section": "General Learning Objectives",
    "text": "General Learning Objectives\n\nUnderstand the landscape of AI and ML explainability techniques\nIdentify explainability needs of data-driven machine learning systems\nRecognise the capabilities and limitations of explainability approaches, both in general and in view of specific use cases\n‚≠ê Apply these skills to real-life AI and ML problems\n‚≠ê Communicate explainability findings through interactive reports and dashboards"
  },
  {
    "objectID": "slides/1_introduction/course.html#practical-learning-objectives",
    "href": "slides/1_introduction/course.html#practical-learning-objectives",
    "title": "Machine Learning Explainability",
    "section": "Practical Learning Objectives",
    "text": "Practical Learning Objectives\n\nIdentify self-contained algorithmic components of explainers and understand their functions\nConnect these building blocks to the explainability requirements unique to the investigated predictive system\nSelect appropriate algorithmic components and tune them to the problem at hand\nEvaluate these building blocks (in this specific context) independently and when joined together to form the final explainer\nInterpret the resulting explanations in view of the uncovered properties and limitations of the bespoke explainability algorithm\n\n\n\nSpecific to explainability approaches"
  },
  {
    "objectID": "slides/1_introduction/course.html#scope",
    "href": "slides/1_introduction/course.html#scope",
    "title": "Machine Learning Explainability",
    "section": "Scope",
    "text": "Scope\n\nIntroduction to explainability\n\nHistory of explainability\nTypes of explanations\nAnte-hoc vs.¬†post-hoc discussion, and information lineage (endogenous and exogenous sources of explanatory information)\nMulti-class explainability\nTaxonomy and classification of explainability approaches\nDefining explainability\nHuman-centred perspective\nEvaluation of explainability techniques"
  },
  {
    "objectID": "slides/1_introduction/course.html#scope-meta-subs.ctd",
    "href": "slides/1_introduction/course.html#scope-meta-subs.ctd",
    "title": "Machine Learning Explainability",
    "section": "Scope ¬†¬†¬†",
    "text": "Scope ¬†¬†¬†\n\nA brief overview of data explainability\n\nData as an (implicit) model\nData summarisation and description\nDimensionality reduction"
  },
  {
    "objectID": "slides/1_introduction/course.html#scope-meta-subs.ctd-1",
    "href": "slides/1_introduction/course.html#scope-meta-subs.ctd-1",
    "title": "Machine Learning Explainability",
    "section": "Scope ¬†¬†¬†",
    "text": "Scope ¬†¬†¬†\n\nTransparent modelling\n\nLinear models\nLogistic models\nGeneralised additive models\nDecision trees\nRule lists and sets; scoped rules\n\\(k\\)-nearest neighbours and \\(k\\)-means"
  },
  {
    "objectID": "slides/1_introduction/course.html#scope-meta-subs.ctd-2",
    "href": "slides/1_introduction/course.html#scope-meta-subs.ctd-2",
    "title": "Machine Learning Explainability",
    "section": "Scope ¬†¬†¬†",
    "text": "Scope ¬†¬†¬†\n\nFeature importance\n\nPartial Dependence-based feature importance\nPermutation Importance\nFeature Interaction"
  },
  {
    "objectID": "slides/1_introduction/course.html#scope-meta-subs.ctd-3",
    "href": "slides/1_introduction/course.html#scope-meta-subs.ctd-3",
    "title": "Machine Learning Explainability",
    "section": "Scope ¬†¬†¬†",
    "text": "Scope ¬†¬†¬†\n\nFeature influence\n\nIndividual Conditional Expectation\nPartial Dependence\nMarginal Effect\nAccumulated Local Effect\nMeta-approaches\n\nLIME (liner surrogate)\nSHAP"
  },
  {
    "objectID": "slides/1_introduction/course.html#scope-meta-subs.ctd-4",
    "href": "slides/1_introduction/course.html#scope-meta-subs.ctd-4",
    "title": "Machine Learning Explainability",
    "section": "Scope ¬†¬†¬†",
    "text": "Scope ¬†¬†¬†\n\nInstance-based explanations\n\nExemplar explanations\nCounterfactuals\nPrototypes and criticisms"
  },
  {
    "objectID": "slides/1_introduction/course.html#scope-meta-subs.ctd-5",
    "href": "slides/1_introduction/course.html#scope-meta-subs.ctd-5",
    "title": "Machine Learning Explainability",
    "section": "Scope ¬†¬†¬†",
    "text": "Scope ¬†¬†¬†\n\nMeta-explainers\n\nsurrogate explainers\n\nlocal, cohort and global\nlinear and tree-based\n\nrules\n\nANCHOR\nRuleFit\n\nSHAP"
  },
  {
    "objectID": "slides/1_introduction/course.html#coursework",
    "href": "slides/1_introduction/course.html#coursework",
    "title": "Machine Learning Explainability",
    "section": "Coursework",
    "text": "Coursework\n\nBring-your-own-project\nExplain a predictive model (you are working with)\n\ndevelop a bespoke explainability suite for a predictive model of your choice (e.g., for a project you are currently working on, or a model accessible via an API)\nuse multiple explainability techniques and identify the sources of explanation (dis)agreements"
  },
  {
    "objectID": "slides/1_introduction/course.html#coursework-meta-subs.ctd",
    "href": "slides/1_introduction/course.html#coursework-meta-subs.ctd",
    "title": "Machine Learning Explainability",
    "section": "Coursework ¬†¬†¬†",
    "text": "Coursework ¬†¬†¬†\n\nDissect an explainability method ‚Äì choose an explainability method, identify its core (algorithmic) building blocks and articulate its assumptions, exploring how these different aspects affect the explanations\nBuild a model-specific or model-agnostic explainer or a transparent model\n\nnew explainability technique (from existing building blocks)\nnew composition of an existing explainability technique\nnew visualisation of an explanation type"
  },
  {
    "objectID": "slides/1_introduction/course.html#coursework-meta-subs.ctd-1",
    "href": "slides/1_introduction/course.html#coursework-meta-subs.ctd-1",
    "title": "Machine Learning Explainability",
    "section": "Coursework ¬†¬†¬†",
    "text": "Coursework ¬†¬†¬†\n\nIndividual or in small groups\nProjects to be presented or demoed in the last class\nSome project ideas\n\n\n\nLet‚Äôs discuss project ideas\nIf you want to brainstorm, we can discuss during office hours\nWe may use the final session for project presentation and discussion"
  },
  {
    "objectID": "slides/1_introduction/course.html#coursework-meta-subs.ctd-2",
    "href": "slides/1_introduction/course.html#coursework-meta-subs.ctd-2",
    "title": "Machine Learning Explainability",
    "section": "Coursework ¬†¬†¬†",
    "text": "Coursework ¬†¬†¬†\n\nObjective: The journey is more important than the outcome\n\n\nReproducibility of the results (research best practice, open science)\nQuality of the investigation\nDue diligence\n\nAssumptions\nChoices (theoretical, algorithmic, implementation and otherwise)\nJustifications\n\nProject results"
  },
  {
    "objectID": "slides/1_introduction/course.html#tools",
    "href": "slides/1_introduction/course.html#tools",
    "title": "Machine Learning Explainability",
    "section": "Tools",
    "text": "Tools\nüíΩ Interactive visualisation / reporting / dashboarding / presentation software\n\nStreamlit\nPlotly Dash\nShiny for Python and R\nQuarto\n\n\n\ntools that may be used for the project presentation / delivery"
  },
  {
    "objectID": "slides/1_introduction/course.html#bibliography",
    "href": "slides/1_introduction/course.html#bibliography",
    "title": "Machine Learning Explainability",
    "section": "Bibliography",
    "text": "Bibliography"
  },
  {
    "objectID": "slides/1_introduction/course.html#questions",
    "href": "slides/1_introduction/course.html#questions",
    "title": "Machine Learning Explainability",
    "section": "Questions",
    "text": "Questions\n\n\n\n\n\nkacper.sokol@rmit.edu.au  k.sokol@bristol.ac.uk"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#explanation-synopsis",
    "href": "slides/3_feature-based/ice.html#explanation-synopsis",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Explanation Synopsis",
    "text": "Explanation Synopsis\n\n\nICE captures the response of a predictive model for a single instance when varying one of its features (Goldstein et al. 2015).\n\n\n\nIt communicates local (with respect to a single instance) feature influence."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#toy-example-numerical-feature",
    "href": "slides/3_feature-based/ice.html#toy-example-numerical-feature",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Toy Example ‚Äì Numerical Feature",
    "text": "Toy Example ‚Äì Numerical Feature"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#toy-example-categorical-feature",
    "href": "slides/3_feature-based/ice.html#toy-example-categorical-feature",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Toy Example ‚Äì Categorical Feature",
    "text": "Toy Example ‚Äì Categorical Feature\n\n\n\nThe lines don‚Äôt show trajectories as these are meaningless for unordered categories.\nThe lines are only useful to discern changes for individual instances."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#method-properties",
    "href": "slides/3_feature-based/ice.html#method-properties",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Method Properties",
    "text": "Method Properties\n\n\n\n\n\n\n\n\nProperty\nIndividual Conditional Expectation\n\n\n\n\nrelation\npost-hoc\n\n\ncompatibility\nmodel-agnostic\n\n\nmodelling\nregression, crisp and probabilistic classification\n\n\nscope\nlocal (per instance; generalises to cohort or global)\n\n\ntarget\nprediction (generalises to model)"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#method-properties-meta-subs.ctd",
    "href": "slides/3_feature-based/ice.html#method-properties-meta-subs.ctd",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Method Properties ¬†¬†¬†",
    "text": "Method Properties ¬†¬†¬†\n\n\n\n\n\n\n\n\nProperty\nIndividual Conditional Expectation\n\n\n\n\ndata\ntabular\n\n\nfeatures\nnumerical and categorical\n\n\nexplanation\nfeature influence (visualisation)\n\n\ncaveats\nfeature correlation, unrealistic instances"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#computing-ice",
    "href": "slides/3_feature-based/ice.html#computing-ice",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Computing ICE",
    "text": "Computing ICE\n\n\n\n\n\n\n\nInput\n\n\n\nSelect a feature to explain\nSelect the explanation target\n\ncrisp classifiers ‚Üí one-vs.-the-rest or all classes\nprobabilistic classifiers ‚Üí (probabilities of) one class\nregressors ‚Üí numerical values\n\nSelect an instance to be explained (or collection thereof)"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#computing-ice-meta-subs.ctd",
    "href": "slides/3_feature-based/ice.html#computing-ice-meta-subs.ctd",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Computing ICE ¬†¬†¬†",
    "text": "Computing ICE ¬†¬†¬†\n\n\n\n\n\n\n\nParameters\n\n\n\nDefine granularity of the explained feature\n\nnumerical attributes ‚Üí select the range ‚Äì minimum and maximum value ‚Äì and the step size of the feature\ncategorical attributes ‚Üí the full set or a subset of possible values"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#computing-ice-meta-subs.ctd-1",
    "href": "slides/3_feature-based/ice.html#computing-ice-meta-subs.ctd-1",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Computing ICE ¬†¬†¬†",
    "text": "Computing ICE ¬†¬†¬†\n\n\n\n\n\n\n\nProcedure\n\n\n\nFor each explained instance create its copy with the value of the explained feature replaced by the range of values determined by the explanation granularity\nPredict the augmented data\nFor each explained instance plot a line that represents the response of the explained model across the entire spectrum of the explained feature\n\n¬†¬†¬† Since the values of the explained feature may not be uniformly distributed in the underlying data set, a rug plot showing the distribution of its feature values can help in interpreting the explanation."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#formulation-fa-square-root-alt",
    "href": "slides/3_feature-based/ice.html#formulation-fa-square-root-alt",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Formulation ¬†¬†¬†",
    "text": "Formulation ¬†¬†¬†\n\\[\nX_{\\mathit{ICE}} \\subseteq \\mathcal{X}\n\\]\n\\[\nV_i = \\{ v_i^{\\mathit{min}} , \\ldots , v_i^{\\mathit{max}} \\}\n\\]\n\\[\nf \\left( x_{\\setminus i} , x_i=v_i \\right) \\;\\; \\forall \\; x \\in X_{\\mathit{ICE}} \\; \\forall \\; v_i \\in V_i\n\\]\n\n\\[\nf \\left( x_{\\setminus i} , x_i=V_i \\right) \\;\\; \\forall \\; x \\in X_{\\mathit{ICE}}\n\\]"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#formulation-fa-square-root-alt-meta-subs.ctd",
    "href": "slides/3_feature-based/ice.html#formulation-fa-square-root-alt-meta-subs.ctd",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Formulation ¬†¬†¬† ¬†¬†¬†",
    "text": "Formulation ¬†¬†¬† ¬†¬†¬†\n\nOriginal notation (Goldstein et al. 2015)\n\n\\[\n\\left\\{ \\left( x_{S}^{(i)} , x_{C}^{(i)} \\right) \\right\\}_{i=1}^N\n\\]\n\n\\[\n\\hat{f}_S^{(i)} = \\hat{f} \\left( x_{S}^{(i)} , x_{C}^{(i)} \\right)\n\\]\n\n\n\\(x_S\\) is stepped through ‚Äì the explained feature\n\\(x_C\\) are the given feature values"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#centred-ice",
    "href": "slides/3_feature-based/ice.html#centred-ice",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Centred ICE",
    "text": "Centred ICE\n\n\nCentres ICE curves by anchoring them at a fixed point, usually the lower end of the explained feature range.\n\n\\[\nf \\left( x_{\\setminus i} , x_i=V_i \\right) -\nf \\left( x_{\\setminus i} , x_i=v_i^{\\mathit{min}} \\right)\n\\;\\; \\forall \\; x \\in X_{\\mathit{ICE}}\n\\]\n\nor\n\n\\[\n\\hat{f} \\left( x_{S}^{(i)} , x_{C}^{(i)} \\right) -\n\\hat{f} \\left( x^{\\star} , x_{C}^{(i)} \\right)\n\\]\n\nHelps to see whether the ICE curves of individual instances behave differently."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#derivative-ice",
    "href": "slides/3_feature-based/ice.html#derivative-ice",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Derivative ICE",
    "text": "Derivative ICE\n\n\nVisualises interaction effects between the explained and remaining features by calculating the partial derivative of the explained model \\(f\\) with respect to the explained feature \\(x_i\\).\n\nWhen no interactions are present, all curves overlap.\nWhen interactions exist, the lines will be heterogeneous."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#derivative-ice-meta-subs.ctd",
    "href": "slides/3_feature-based/ice.html#derivative-ice-meta-subs.ctd",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Derivative ICE ¬†¬†¬†",
    "text": "Derivative ICE ¬†¬†¬†\n\n\\[\nf \\left( x_{\\setminus i} , x_i \\right) =\ng \\left( x_i \\right) + h \\left( x_{\\setminus i} \\right)\n\\;\\; \\text{so that} \\;\\;\n\\frac{\\partial f(x)}{\\partial x_i} = g^\\prime(x_i)\n\\]\n\nor\n\n\\[\n\\hat{f} \\left( x_{S} , x_{C} \\right) =\ng \\left( x_{S} \\right) + h \\left( x_{C} \\right)\n\\;\\; \\text{so that} \\;\\;\n\\frac{\\partial \\hat{f}(x)}{\\partial x_{S}} = g^\\prime(x_{S})\n\\]\n\n\nThis assumes no interaction (correlation) between the inspected / explained and the remaining features.\n(Derivatives) communicates the rate and direction of changes in each ICE line."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#ice-of-a-single-instance",
    "href": "slides/3_feature-based/ice.html#ice-of-a-single-instance",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "ICE of a Single Instance",
    "text": "ICE of a Single Instance"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#ice-of-a-data-collection",
    "href": "slides/3_feature-based/ice.html#ice-of-a-data-collection",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "ICE of a Data Collection",
    "text": "ICE of a Data Collection"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#centred-ice-1",
    "href": "slides/3_feature-based/ice.html#centred-ice-1",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Centred ICE",
    "text": "Centred ICE"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#derivative-ice-1",
    "href": "slides/3_feature-based/ice.html#derivative-ice-1",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Derivative ICE",
    "text": "Derivative ICE"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#out-of-distribution-impossible-instances",
    "href": "slides/3_feature-based/ice.html#out-of-distribution-impossible-instances",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Out-of-distribution (Impossible) Instances",
    "text": "Out-of-distribution (Impossible) Instances"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#out-of-distribution-impossible-instances-meta-subs.ctd",
    "href": "slides/3_feature-based/ice.html#out-of-distribution-impossible-instances-meta-subs.ctd",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Out-of-distribution (Impossible) Instances ¬†¬†¬†",
    "text": "Out-of-distribution (Impossible) Instances"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#out-of-distribution-impossible-instances-meta-subs.ctd-1",
    "href": "slides/3_feature-based/ice.html#out-of-distribution-impossible-instances-meta-subs.ctd-1",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Out-of-distribution (Impossible) Instances ¬†¬†¬†",
    "text": "Out-of-distribution (Impossible) Instances ¬†¬†¬†\n\n\nNote the gaps in the feature range, which is due to how scikit-learn computes the range for ICE calculation."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#out-of-distribution-impossible-instances-meta-subs.ctd-2",
    "href": "slides/3_feature-based/ice.html#out-of-distribution-impossible-instances-meta-subs.ctd-2",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Out-of-distribution (Impossible) Instances ¬†¬†¬†",
    "text": "Out-of-distribution (Impossible) Instances"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#feature-correlation",
    "href": "slides/3_feature-based/ice.html#feature-correlation",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Feature Correlation",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#feature-correlation-meta-subs.ctd",
    "href": "slides/3_feature-based/ice.html#feature-correlation-meta-subs.ctd",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation ¬†¬†¬†\n\n\n\nGo back to the previous plot and show that for negative coefficient features the probability decreases; but for positive coefficient features it increases.\nThe rate of chagne depends on the magnitude of the coefficient.\nCaveat: The features were not normalised to the same range so these aren‚Äôt really directly comparable."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#feature-correlation-meta-subs.ctd-1",
    "href": "slides/3_feature-based/ice.html#feature-correlation-meta-subs.ctd-1",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#target-correlation",
    "href": "slides/3_feature-based/ice.html#target-correlation",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Target Correlation",
    "text": "Target Correlation"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#feature-2-1-correlation-small",
    "href": "slides/3_feature-based/ice.html#feature-2-1-correlation-small",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Feature 2 & 1 Correlation (small)",
    "text": "Feature 2 & 1 Correlation (small)"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#feature-2-1-correlation-small-meta-subs.ctd",
    "href": "slides/3_feature-based/ice.html#feature-2-1-correlation-small-meta-subs.ctd",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Feature 2 & 1 Correlation (small) ¬†¬†¬†",
    "text": "Feature 2 & 1 Correlation (small) ¬†¬†¬†\n\n\n\nWhen using the features with least correlation, the behaviour is most unlike the full 4 feature plot.\nAs we will see with the other (correlated) figures, the behaviour is almost unchanged."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#feature-2-3-correlation-medium",
    "href": "slides/3_feature-based/ice.html#feature-2-3-correlation-medium",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Feature 2 & 3 Correlation (medium)",
    "text": "Feature 2 & 3 Correlation (medium)"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#feature-2-3-correlation-medium-meta-subs.ctd",
    "href": "slides/3_feature-based/ice.html#feature-2-3-correlation-medium-meta-subs.ctd",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Feature 2 & 3 Correlation (medium) ¬†¬†¬†",
    "text": "Feature 2 & 3 Correlation (medium)"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#feature-2-4-correlation-medium",
    "href": "slides/3_feature-based/ice.html#feature-2-4-correlation-medium",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Feature 2 & 4 Correlation (medium)",
    "text": "Feature 2 & 4 Correlation (medium)"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#feature-2-4-correlation-medium-meta-subs.ctd",
    "href": "slides/3_feature-based/ice.html#feature-2-4-correlation-medium-meta-subs.ctd",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Feature 2 & 4 Correlation (medium) ¬†¬†¬†",
    "text": "Feature 2 & 4 Correlation (medium)"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#feature-3-4-correlation-high",
    "href": "slides/3_feature-based/ice.html#feature-3-4-correlation-high",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Feature 3 & 4 Correlation (high)",
    "text": "Feature 3 & 4 Correlation (high)"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#feature-3-4-correlation-high-meta-subs.ctd",
    "href": "slides/3_feature-based/ice.html#feature-3-4-correlation-high-meta-subs.ctd",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Feature 3 & 4 Correlation (high) ¬†¬†¬†",
    "text": "Feature 3 & 4 Correlation (high)"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#pros-fa-plus-square",
    "href": "slides/3_feature-based/ice.html#pros-fa-plus-square",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Pros ¬†¬†¬†",
    "text": "Pros ¬†¬†¬†\n\nEasy to generate and interpret\nSpanning multiple instances allows to capture the diversity (heterogeneity) of the model‚Äôs behaviour"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#cons-fa-minus-square",
    "href": "slides/3_feature-based/ice.html#cons-fa-minus-square",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Cons ¬†¬†¬†",
    "text": "Cons ¬†¬†¬†\n\nAssumes feature independence, which is often unreasonable\nICE may not reflect the true behaviour of the model since it displays the behaviour of the model for unrealistic instances\nMay be unreliable for certain values of the explained feature when its values are not uniformly distributed (abated by a rug plot)\nLimited to explaining one feature at a time"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#caveats-fa-skull",
    "href": "slides/3_feature-based/ice.html#caveats-fa-skull",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Caveats ¬†¬†¬†",
    "text": "Caveats ¬†¬†¬†\n\nAveraging ICEs gives Partial Dependence (PD)\nGenerating ICEs may be computationally expensive for large sets of data and wide feature intervals with a small ‚Äúinspection‚Äù step\nComputational complexity: \\(\\mathcal{O} \\left( n \\times d \\right)\\), where\n\n\\(n\\) is the number of instances in the designated data set and\n\\(d\\) is the number of steps within the designated feature interval"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#causal-interpretation",
    "href": "slides/3_feature-based/ice.html#causal-interpretation",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Causal Interpretation",
    "text": "Causal Interpretation\nUnder certain (quite restrictive) assumptions, ICE is admissible to a causal interpretation (Zhao and Hastie 2021).\nSee  Causal Interpretation of Partial Dependence (PD) for more detail."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#related-techniques",
    "href": "slides/3_feature-based/ice.html#related-techniques",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Related Techniques",
    "text": "Related Techniques\n\nPartial Dependence (PD)\n\n ¬†¬†¬† Model-focused (global) ‚Äúversion‚Äù of Individual Conditional Expectation, which is calculated by averaging ICE across a collection of data points (Friedman 2001). It communicates the average influence of a specific feature value on the model‚Äôs prediction by fixing the value of this feature across a designated set of instances."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#related-techniques-meta-subs.ctd",
    "href": "slides/3_feature-based/ice.html#related-techniques-meta-subs.ctd",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Related Techniques ¬†¬†¬†",
    "text": "Related Techniques ¬†¬†¬†\n\nMarginal Effect (Marginal Plots or M-Plots)\n\n ¬†¬†¬† It communicates the influence of a specific feature value ‚Äì or similar values, i.e., an interval around the selected value ‚Äì on the model‚Äôs prediction by only considering relevant instances found in the designated data set. It is calculated as the average prediction of these instances."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#related-techniques-meta-subs.ctd-1",
    "href": "slides/3_feature-based/ice.html#related-techniques-meta-subs.ctd-1",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Related Techniques ¬†¬†¬†",
    "text": "Related Techniques ¬†¬†¬†\n\nAccumulated Local Effect (ALE)\n\n ¬†¬†¬† It communicates the influence of a specific feature value on the model‚Äôs prediction by quantifying the average (accumulated) difference between the predictions at the boundaries of a (small) fixed interval around the selected feature value (Apley and Zhu 2020). It is calculated by replacing the value of the explained feature with the interval boundaries for instances found in the designated data set whose value of this feature is within the specified range."
  },
  {
    "objectID": "slides/3_feature-based/ice.html#implementations",
    "href": "slides/3_feature-based/ice.html#implementations",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Implementations",
    "text": "Implementations\n\n\n\n\n\n\n\n Python\n R\n\n\n\n\nscikit-learn (>=0.24.0)\niml\n\n\nPyCEbox\nICEbox\n\n\nalibi\npdp\n\n\n\nDALEX"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#further-reading",
    "href": "slides/3_feature-based/ice.html#further-reading",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Further Reading",
    "text": "Further Reading\n\nICE paper (Goldstein et al. 2015)\nInterpretable Machine Learning book\nscikit-learn example\nFAT Forensics example and tutorial"
  },
  {
    "objectID": "slides/3_feature-based/ice.html#bibliography",
    "href": "slides/3_feature-based/ice.html#bibliography",
    "title": "Individual Conditional Expectation (ICE)",
    "section": "Bibliography",
    "text": "Bibliography\n\n\nApley, Daniel W, and Jingyu Zhu. 2020. ‚ÄúVisualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.‚Äù Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82 (4): 1059‚Äì86.\n\n\nFriedman, Jerome H. 2001. ‚ÄúGreedy Function Approximation: A Gradient Boosting Machine.‚Äù Annals of Statistics, 1189‚Äì1232.\n\n\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. ‚ÄúPeeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.‚Äù Journal of Computational and Graphical Statistics 24 (1): 44‚Äì65.\n\n\nZhao, Qingyuan, and Trevor Hastie. 2021. ‚ÄúCausal Interpretations of Black-Box Models.‚Äù Journal of Business & Economic Statistics 39 (1): 272‚Äì81."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#explanation-synopsis",
    "href": "slides/3_feature-based/pd.html#explanation-synopsis",
    "title": "Partial Dependence (PD)",
    "section": "Explanation Synopsis",
    "text": "Explanation Synopsis\n\n\nPD captures the average response of a predictive model for a collection of instances when varying one of their features (Friedman 2001).\n\n\n\nIt communicates global (with respect to the entire explained model) feature influence."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#toy-example-numerical-feature",
    "href": "slides/3_feature-based/pd.html#toy-example-numerical-feature",
    "title": "Partial Dependence (PD)",
    "section": "Toy Example ‚Äì Numerical Feature",
    "text": "Toy Example ‚Äì Numerical Feature"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#toy-example-categorical-feature",
    "href": "slides/3_feature-based/pd.html#toy-example-categorical-feature",
    "title": "Partial Dependence (PD)",
    "section": "Toy Example ‚Äì Categorical Feature",
    "text": "Toy Example ‚Äì Categorical Feature\n\n\n\nSometimes you will see this visualised as a bar chart.\nYou could also use box plots."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#method-properties",
    "href": "slides/3_feature-based/pd.html#method-properties",
    "title": "Partial Dependence (PD)",
    "section": "Method Properties",
    "text": "Method Properties\n\n\n\n\n\n\n\n\nProperty\nPartial Dependence\n\n\n\n\nrelation\npost-hoc\n\n\ncompatibility\nmodel-agnostic\n\n\nmodelling\nregression, crisp and probabilistic classification\n\n\nscope\nglobal (per data set; generalises to cohort)\n\n\ntarget\nmodel (set of predictions)"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#method-properties-meta-subs.ctd",
    "href": "slides/3_feature-based/pd.html#method-properties-meta-subs.ctd",
    "title": "Partial Dependence (PD)",
    "section": "Method Properties ¬†¬†¬†",
    "text": "Method Properties ¬†¬†¬†\n\n\n\n\n\n\n\n\nProperty\nPartial Dependence\n\n\n\n\ndata\ntabular\n\n\nfeatures\nnumerical and categorical\n\n\nexplanation\nfeature influence (visualisation)\n\n\ncaveats\nfeature correlation, unrealistic instances, heterogeneous model response"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#computing-pd",
    "href": "slides/3_feature-based/pd.html#computing-pd",
    "title": "Partial Dependence (PD)",
    "section": "Computing PD",
    "text": "Computing PD\n\n\n\n\n\n\n\nInput\n\n\n\nSelect a feature to explain\nSelect the explanation target\n\ncrisp classifiers ‚Üí one(-vs.-the-rest) or all classes\nprobabilistic classifiers ‚Üí (probabilities of) one class\nregressors ‚Üí numerical values\n\nSelect a collection of instances to generate the explanation"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#computing-pd-meta-subs.ctd",
    "href": "slides/3_feature-based/pd.html#computing-pd-meta-subs.ctd",
    "title": "Partial Dependence (PD)",
    "section": "Computing PD ¬†¬†¬†",
    "text": "Computing PD ¬†¬†¬†\n\n\n\n\n\n\n\nParameters\n\n\n\nDefine granularity of the explained feature\n\nnumerical attributes ‚Üí select the range ‚Äì minimum and maximum value ‚Äì and the step size of the feature\ncategorical attributes ‚Üí the full set or a subset of possible values"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#computing-pd-meta-subs.ctd-1",
    "href": "slides/3_feature-based/pd.html#computing-pd-meta-subs.ctd-1",
    "title": "Partial Dependence (PD)",
    "section": "Computing PD ¬†¬†¬†",
    "text": "Computing PD ¬†¬†¬†\n\n\n\n\n\n\n\nProcedure\n\n\n\nFor each instance in the designated data set create its copy with the value of the explained feature replaced by the range of values determined by the explanation granularity\nPredict the augmented data\nGenerate and plot Partial Dependence\n\nfor crisp classifiers count the number of each unique prediction at each value of the explained feature across all the instances; visualise PD either as a count or proportion using separate line for each class or using a stacked bar chart\nfor probabilistic classifiers (per class) and regressors average the response of the model at each value of the explained feature across all the instances; visualise PD as a line\n\n\n¬†¬†¬† Since the values of the explained feature may not be uniformly distributed in the underlying data set, a rug plot showing the distribution of its feature values can help in interpreting the explanation."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#formulation-fa-square-root-alt",
    "href": "slides/3_feature-based/pd.html#formulation-fa-square-root-alt",
    "title": "Partial Dependence (PD)",
    "section": "Formulation ¬†¬†¬†",
    "text": "Formulation ¬†¬†¬†\n\\[\nX_{\\mathit{PD}} \\subseteq \\mathcal{X}\n\\]\n\\[\nV_i = \\{ v_i^{\\mathit{min}} , \\ldots , v_i^{\\mathit{max}} \\}\n\\]\n\\[\n\\mathit{PD}_i =\n\\mathbb{E}_{X_{\\setminus i}} \\left[ f \\left( X_{\\setminus i} , x_i=v_i \\right) \\right] =\n\\int f \\left( X_{\\setminus i} , x_i=v_i \\right) \\; d \\mathbb{P} ( X_{\\setminus i} )\n\\;\\; \\forall \\; v_i \\in V_i\n\\]\n\n\\[\n\\mathit{PD}_i =\n\\mathbb{E}_{X_{\\setminus i}} \\left[ f \\left( X_{\\setminus i} , x_i=V_i \\right) \\right] =\n\\int f \\left( X_{\\setminus i} , x_i=V_i \\right) \\; d \\mathbb{P} ( X_{\\setminus i} )\n\\]"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#formulation-fa-square-root-alt-meta-subs.ctd",
    "href": "slides/3_feature-based/pd.html#formulation-fa-square-root-alt-meta-subs.ctd",
    "title": "Partial Dependence (PD)",
    "section": "Formulation ¬†¬†¬† ¬†¬†¬†",
    "text": "Formulation ¬†¬†¬† ¬†¬†¬†\n\nBased on the ICE notation (Goldstein et al. 2015)\n\n\\[\n\\left\\{ \\left( x_{S}^{(i)} , x_{C}^{(i)} \\right) \\right\\}_{i=1}^N\n\\]\n\n\\[\n\\hat{f}_S =\n\\mathbb{E}_{X_{C}} \\left[ \\hat{f} \\left( x_{S} , X_{C} \\right) \\right] =\n\\int \\hat{f} \\left( x_{S} , X_{C} \\right) \\; d \\mathbb{P} ( X_{C} )\n\\]\n\n\n\\(x_S\\) is stepped through ‚Äì the explained feature\n\\(x_C\\) are the given feature values\n\\(X_C\\) is the random variable\nMarginalising the predictions over the distribution of the given features yields dependence between the explained feature(s) (including any interactions) and predictions."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#approximation-fa-desktop",
    "href": "slides/3_feature-based/pd.html#approximation-fa-desktop",
    "title": "Partial Dependence (PD)",
    "section": "Approximation ¬†¬†¬†",
    "text": "Approximation ¬†¬†¬†\n\n(Monte Carlo approximation)\n\n\\[\n\\mathit{PD}_i \\approx\n\\frac{1}{|X_{\\mathit{PD}}|} \\sum_{x \\in X_{\\mathit{PD}}}\nf \\left( x_ {\\setminus i} , x_i=v_i \\right)\n\\]\n\n\\[\n\\hat{f}_S \\approx\n\\frac{1}{N} \\sum_{i = 1}^N\n\\hat{f} \\left( x_{S} , x_{C}^{(i)} \\right)\n\\]"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#centred-pd",
    "href": "slides/3_feature-based/pd.html#centred-pd",
    "title": "Partial Dependence (PD)",
    "section": "Centred PD",
    "text": "Centred PD\n\n\nCentres PD curve by anchoring it at a fixed point, usually the lower end of the explained feature range. It is helpful when working with  Centred ICE.\n\n\\[\n\\mathbb{E}_{X_{\\setminus i}} \\left[ f \\left( X_{\\setminus i} , x_i=V_i \\right) \\right] -\n\\mathbb{E}_{X_{\\setminus i}} \\left[ f \\left( X_{\\setminus i} , x_i=v_i^{\\mathit{min}} \\right) \\right]\n\\]\n\nor\n\n\\[\n\\mathbb{E}_{X_{C}} \\left[ \\hat{f} \\left( x_{S}^{(i)} , X_{C} \\right) \\right] -\n\\mathbb{E}_{X_{C}} \\left[ \\hat{f} \\left( x^{\\star} , X_{C} \\right) \\right]\n\\]\n\n\nHelps to see whether the underlying ICE curves of individual instances behave differently."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd-based-feature-importance",
    "href": "slides/3_feature-based/pd.html#pd-based-feature-importance",
    "title": "Partial Dependence (PD)",
    "section": "PD-based Feature Importance",
    "text": "PD-based Feature Importance\n\n\nImportance of a feature can be derived from a PD curve by assessing its flatness (Greenwell, Boehmke, and McCarthy 2018). A flat PD line indicates that the model is not overly sensitive to the values of the selected feature, hence it is not important for the model‚Äôs decisions.\n\n\n\n\n\n\n\n\nCaveat\n\n\nSimilar to PD plots, this formulation of feature importance will not capture heterogeneity of individual instances that underlie the PD calculation."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd-based-feature-importance-meta-subs.ctd",
    "href": "slides/3_feature-based/pd.html#pd-based-feature-importance-meta-subs.ctd",
    "title": "Partial Dependence (PD)",
    "section": "PD-based Feature Importance ¬†¬†¬†",
    "text": "PD-based Feature Importance ¬†¬†¬†\n\nFor example, for numerical features, it can be defined as the (standard) deviation of PD measurement for each unique value of the explained feature from the average PD.\n\n\\[\nI_{\\mathit{PD}} (i) = \\sqrt{\n    \\frac{1}{|V_i| - 1}\n    \\sum_{v_i \\in V_i} \\left(\n        \\mathit{PD}_i - \\underbrace{\n            \\frac{1}{|V_i|}\n            \\sum_{v_i \\in V_i} \\mathit{PD}_i\n        }_{\\text{average PD}}\n    \\right)^2\n}\n\\]"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd-based-feature-importance-meta-subs.ctd-1",
    "href": "slides/3_feature-based/pd.html#pd-based-feature-importance-meta-subs.ctd-1",
    "title": "Partial Dependence (PD)",
    "section": "PD-based Feature Importance ¬†¬†¬†",
    "text": "PD-based Feature Importance ¬†¬†¬†\n\nFor categorical features, it can be defined as the range statistic divided by four (range rule) of PD values, which provides a rough estimate of the standard deviation.\n\n\\[\nI_{\\mathit{PD}} (i) = \\frac{\n    \\max_{V_i} \\; \\mathit{PD}_i - \\min_{V_i} \\; \\mathit{PD}_i\n}{\n    4\n}\n\\]\n\n\n\n\n\n\n\nFormula\n\n\nFor the normal distribution, 95% of data is within ¬±2 standard deviations. Assuming a relatively small sample size, the range is likely to come from within this 95% interval. Therefore, the range divided by 4 roughly (under)estimates the standard deviation."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd-based-feature-importance-meta-subs.ctd-2",
    "href": "slides/3_feature-based/pd.html#pd-based-feature-importance-meta-subs.ctd-2",
    "title": "Partial Dependence (PD)",
    "section": "PD-based Feature Importance ¬†¬†¬†",
    "text": "PD-based Feature Importance ¬†¬†¬†\n\nBased on the ICE notation (Goldstein et al. 2015), where \\(K\\) is the number of unique values \\(x_S^{(k)}\\) of the explained feature \\(x_S\\)\n\n\\[\nI_{\\mathit{PD}} (x_S) = \\sqrt{\n    \\frac{1}{K - 1}\n    \\sum_{k=1}^K \\left(\n        \\hat{f}_S(x^{(k)}_S) - \\underbrace{\n            \\frac{1}{K}\n            \\sum_{k=1}^K \\hat{f}_S(x^{(k)}_S)\n        }_{\\text{average PD}}\n    \\right)^2\n}\n\\]\n\\[\nI_{\\mathit{PD}} (x_S) = \\frac{\n    \\max_{k} \\; \\hat{f}_S(x^{(k)}_S) - \\min_{k} \\; \\hat{f}_S(x^{(k)}_S)\n}{\n    4\n}\n\\]"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd",
    "href": "slides/3_feature-based/pd.html#pd",
    "title": "Partial Dependence (PD)",
    "section": "PD",
    "text": "PD"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd-with-standard-deviation",
    "href": "slides/3_feature-based/pd.html#pd-with-standard-deviation",
    "title": "Partial Dependence (PD)",
    "section": "PD with Standard Deviation",
    "text": "PD with Standard Deviation"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd-with-ice",
    "href": "slides/3_feature-based/pd.html#pd-with-ice",
    "title": "Partial Dependence (PD)",
    "section": "PD with ICE",
    "text": "PD with ICE"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd-with-standard-deviation-ice",
    "href": "slides/3_feature-based/pd.html#pd-with-standard-deviation-ice",
    "title": "Partial Dependence (PD)",
    "section": "PD with Standard Deviation & ICE",
    "text": "PD with Standard Deviation & ICE"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#centred-pd-with-standard-deviation-ice",
    "href": "slides/3_feature-based/pd.html#centred-pd-with-standard-deviation-ice",
    "title": "Partial Dependence (PD)",
    "section": "Centred PD (with Standard Deviation & ICE)",
    "text": "Centred PD (with Standard Deviation & ICE)"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd-for-two-numerical-features",
    "href": "slides/3_feature-based/pd.html#pd-for-two-numerical-features",
    "title": "Partial Dependence (PD)",
    "section": "PD for Two (Numerical) Features",
    "text": "PD for Two (Numerical) Features"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd-for-crisp-classifiers",
    "href": "slides/3_feature-based/pd.html#pd-for-crisp-classifiers",
    "title": "Partial Dependence (PD)",
    "section": "PD for Crisp Classifiers",
    "text": "PD for Crisp Classifiers"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd-for-crisp-classifiers-meta-subs.ctd",
    "href": "slides/3_feature-based/pd.html#pd-for-crisp-classifiers-meta-subs.ctd",
    "title": "Partial Dependence (PD)",
    "section": "PD for Crisp Classifiers ¬†¬†¬†",
    "text": "PD for Crisp Classifiers ¬†¬†¬†\n\n\n\nGaps are there because scikit-learn does not sample the explained feature uniformly."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pd-based-feature-importance-1",
    "href": "slides/3_feature-based/pd.html#pd-based-feature-importance-1",
    "title": "Partial Dependence (PD)",
    "section": "PD-based Feature Importance",
    "text": "PD-based Feature Importance"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#out-of-distribution-impossible-instances",
    "href": "slides/3_feature-based/pd.html#out-of-distribution-impossible-instances",
    "title": "Partial Dependence (PD)",
    "section": "Out-of-distribution (Impossible) Instances",
    "text": "Out-of-distribution (Impossible) Instances\n\n\n\n\n\n\n\n\n\n\n\nFor more exmaples see the  Out-of-distribution (Impossible) Instances topic for ICE."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#feature-correlation",
    "href": "slides/3_feature-based/pd.html#feature-correlation",
    "title": "Partial Dependence (PD)",
    "section": "Feature Correlation",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#feature-correlation-meta-subs.ctd",
    "href": "slides/3_feature-based/pd.html#feature-correlation-meta-subs.ctd",
    "title": "Partial Dependence (PD)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#feature-correlation-meta-subs.ctd-1",
    "href": "slides/3_feature-based/pd.html#feature-correlation-meta-subs.ctd-1",
    "title": "Partial Dependence (PD)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#feature-correlation-meta-subs.ctd-2",
    "href": "slides/3_feature-based/pd.html#feature-correlation-meta-subs.ctd-2",
    "title": "Partial Dependence (PD)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation ¬†¬†¬†\n\n\n\n\n\n\n\n\n\n\n\nFor more exmaples see the  Feature Correlation topic for ICE."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#heterogeneous-influence",
    "href": "slides/3_feature-based/pd.html#heterogeneous-influence",
    "title": "Partial Dependence (PD)",
    "section": "Heterogeneous Influence",
    "text": "Heterogeneous Influence"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#heterogeneous-influence-meta-subs.ctd",
    "href": "slides/3_feature-based/pd.html#heterogeneous-influence-meta-subs.ctd",
    "title": "Partial Dependence (PD)",
    "section": "Heterogeneous Influence ¬†¬†¬†",
    "text": "Heterogeneous Influence"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#pros-fa-plus-square",
    "href": "slides/3_feature-based/pd.html#pros-fa-plus-square",
    "title": "Partial Dependence (PD)",
    "section": "Pros ¬†¬†¬†",
    "text": "Pros ¬†¬†¬†\n\nEasy to generate and interpret\nCan be derived from ICEs\nCan be used to compute feature importance"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#cons-fa-minus-square",
    "href": "slides/3_feature-based/pd.html#cons-fa-minus-square",
    "title": "Partial Dependence (PD)",
    "section": "Cons ¬†¬†¬†",
    "text": "Cons ¬†¬†¬†\n\nAssumes feature independence, which is often unreasonable\nPD may not reflect the true behaviour of the model since it based upon the behaviour of the model for unrealistic instances\nMay be unreliable for certain values of the explained feature when its values are not uniformly distributed (abated by a rug plot)\nLimited to explaining two feature at a time\nDoes not capture the diversity (heterogeneity) of the model‚Äôs behaviour for the individual instances used for PD calculation (abated by displaying the underlying ICE lines)"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#caveats-fa-skull",
    "href": "slides/3_feature-based/pd.html#caveats-fa-skull",
    "title": "Partial Dependence (PD)",
    "section": "Caveats ¬†¬†¬†",
    "text": "Caveats ¬†¬†¬†\n\nPD is derived by averaging ICEs\nGenerating PD may be computationally expensive for large sets of data and wide feature intervals with a small ‚Äúinspection‚Äù step\nComputational complexity: \\(\\mathcal{O} \\left( n \\times d \\right)\\), where\n\n\\(n\\) is the number of instances in the designated data set and\n\\(d\\) is the number of steps within the designated feature interval"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#causal-interpretation",
    "href": "slides/3_feature-based/pd.html#causal-interpretation",
    "title": "Partial Dependence (PD)",
    "section": "Causal Interpretation",
    "text": "Causal Interpretation\n\nZhao and Hastie (2021) noticed similarity in the formulation of Partial Dependence and Pearl‚Äôs back-door criterion (Pearl, Glymour, and Jewell 2016), allowing for a causal interpretation of PD under quite restrictive assumptions:\n\nthe explained predictive model is a good (truthful) approximation of the underlying data generation process;\ndetailed domain knowledge is available, allowing us to assess the causal structure of the problem and verify the back-door criterion (see below); and\nthe set of features complementary to the explained attribute satisfies the back-door criterion, i.e., none of the complementary features are causal descendant of the explained attribute."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#causal-interpretation-meta-subs.ctd",
    "href": "slides/3_feature-based/pd.html#causal-interpretation-meta-subs.ctd",
    "title": "Partial Dependence (PD)",
    "section": "Causal Interpretation ¬†¬†¬†",
    "text": "Causal Interpretation ¬†¬†¬†\n\nBy interveening on the explained feature, we measure the change in the model‚Äôs output, allowing us to analyse the causal relationship between the two.\n\n\n\n\n\n\n\nCaveat\n\n\nIn principle, the causal relationship is with respect to the explained model, and not the underlying phenomenon (that generates the data)."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#related-techniques",
    "href": "slides/3_feature-based/pd.html#related-techniques",
    "title": "Partial Dependence (PD)",
    "section": "Related Techniques",
    "text": "Related Techniques\n\nIndividual Conditional Expectation (ICE)\n\n ¬†¬†¬† Instance-focused (local) ‚Äúversion‚Äù of Partial Dependence, which communicates the influence of a specific feature value on the model‚Äôs prediction by fixing the value of this feature for a single data point (Goldstein et al. 2015)."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#related-techniques-meta-subs.ctd",
    "href": "slides/3_feature-based/pd.html#related-techniques-meta-subs.ctd",
    "title": "Partial Dependence (PD)",
    "section": "Related Techniques ¬†¬†¬†",
    "text": "Related Techniques ¬†¬†¬†\n\nMarginal Effect (Marginal Plots or M-Plots)\n\n ¬†¬†¬† It communicates the influence of a specific feature value ‚Äì or similar values, i.e., an interval around the selected value ‚Äì on the model‚Äôs prediction by only considering relevant instances found in the designated data set. It is calculated as the average prediction of these instances."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#related-techniques-meta-subs.ctd-1",
    "href": "slides/3_feature-based/pd.html#related-techniques-meta-subs.ctd-1",
    "title": "Partial Dependence (PD)",
    "section": "Related Techniques ¬†¬†¬†",
    "text": "Related Techniques ¬†¬†¬†\n\nAccumulated Local Effect (ALE)\n\n ¬†¬†¬† It communicates the influence of a specific feature value on the model‚Äôs prediction by quantifying the average (accumulated) difference between the predictions at the boundaries of a (small) fixed interval around the selected feature value (Apley and Zhu 2020). It is calculated by replacing the value of the explained feature with the interval boundaries for instances found in the designated data set whose value of this feature is within the specified range."
  },
  {
    "objectID": "slides/3_feature-based/pd.html#implementations",
    "href": "slides/3_feature-based/pd.html#implementations",
    "title": "Partial Dependence (PD)",
    "section": "Implementations",
    "text": "Implementations\n\n\n\n\n\n\n\n Python\n R\n\n\n\n\nscikit-learn (>=0.24.0)\niml\n\n\nPyCEbox\nICEbox\n\n\nPDPbox\npdp\n\n\nInterpretML\nDALEX\n\n\nSkater\n\n\n\nalibi"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#further-reading",
    "href": "slides/3_feature-based/pd.html#further-reading",
    "title": "Partial Dependence (PD)",
    "section": "Further Reading",
    "text": "Further Reading\n\nPD paper (Friedman 2001)\nInterpretable Machine Learning book\nExplanatory Model Analysis book\nKaggle course\nscikit-learn example\nFAT Forensics example and tutorial\nInterpretML example"
  },
  {
    "objectID": "slides/3_feature-based/pd.html#bibliography",
    "href": "slides/3_feature-based/pd.html#bibliography",
    "title": "Partial Dependence (PD)",
    "section": "Bibliography",
    "text": "Bibliography\n\n\nApley, Daniel W, and Jingyu Zhu. 2020. ‚ÄúVisualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.‚Äù Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82 (4): 1059‚Äì86.\n\n\nFriedman, Jerome H. 2001. ‚ÄúGreedy Function Approximation: A Gradient Boosting Machine.‚Äù Annals of Statistics, 1189‚Äì1232.\n\n\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. ‚ÄúPeeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.‚Äù Journal of Computational and Graphical Statistics 24 (1): 44‚Äì65.\n\n\nGreenwell, Brandon M, Bradley C Boehmke, and Andrew J McCarthy. 2018. ‚ÄúA Simple and Effective Model-Based Variable Importance Measure.‚Äù arXiv Preprint arXiv:1805.04755.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P Jewell. 2016. Causal Inference in Statistics: A Primer. John Wiley & Sons.\n\n\nZhao, Qingyuan, and Trevor Hastie. 2021. ‚ÄúCausal Interpretations of Black-Box Models.‚Äù Journal of Business & Economic Statistics 39 (1): 272‚Äì81."
  },
  {
    "objectID": "slides/3_feature-based/me.html#explanation-synopsis",
    "href": "slides/3_feature-based/me.html#explanation-synopsis",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Explanation Synopsis",
    "text": "Explanation Synopsis\n\n\nME captures the average response of a predictive model across a collection of instances (taken from a designated data set) for a specific value of a selected feature (found in the aforementioned data set) (Apley and Zhu 2020). This measure can be relaxed by including similar feature values determined by a fixed interval around the selected value.\n\n\n\nIt communicates global (with respect to the entire explained model) feature influence."
  },
  {
    "objectID": "slides/3_feature-based/me.html#rationale",
    "href": "slides/3_feature-based/me.html#rationale",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Rationale",
    "text": "Rationale\n\n\nME improves upon  Partial Dependence (PD) (Friedman 2001) by ensuring that the influence estimates are based on realistic instances (thus respecting feature correlation), making the explanatory insights more truthful.\n\n\n\n\n\n\n\n\nMethod‚Äôs Name\n\n\nNote that even though the Marginal Effect name suggests that these explanations are based on the marginal distribution of the selected feature, they are actually derived from its conditional distribution."
  },
  {
    "objectID": "slides/3_feature-based/me.html#toy-example-strict-me-numerical-feature",
    "href": "slides/3_feature-based/me.html#toy-example-strict-me-numerical-feature",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Toy Example ‚Äì Strict ME ‚Äì Numerical Feature",
    "text": "Toy Example ‚Äì Strict ME ‚Äì Numerical Feature"
  },
  {
    "objectID": "slides/3_feature-based/me.html#toy-example-relaxed-me-numerical-feature",
    "href": "slides/3_feature-based/me.html#toy-example-relaxed-me-numerical-feature",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Toy Example ‚Äì Relaxed ME ‚Äì Numerical Feature",
    "text": "Toy Example ‚Äì Relaxed ME ‚Äì Numerical Feature"
  },
  {
    "objectID": "slides/3_feature-based/me.html#toy-example-strict-me-categorical-feature",
    "href": "slides/3_feature-based/me.html#toy-example-strict-me-categorical-feature",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Toy Example ‚Äì Strict ME ‚Äì Categorical Feature",
    "text": "Toy Example ‚Äì Strict ME ‚Äì Categorical Feature"
  },
  {
    "objectID": "slides/3_feature-based/me.html#method-properties",
    "href": "slides/3_feature-based/me.html#method-properties",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Method Properties",
    "text": "Method Properties\n\n\n\n\n\n\n\n\nProperty\nMarginal Effect\n\n\n\n\nrelation\npost-hoc\n\n\ncompatibility\nmodel-agnostic\n\n\nmodelling\nregression, crisp and probabilistic classification\n\n\nscope\nglobal (per data set; generalises to cohort)\n\n\ntarget\nmodel (set of predictions)"
  },
  {
    "objectID": "slides/3_feature-based/me.html#method-properties-meta-subs.ctd",
    "href": "slides/3_feature-based/me.html#method-properties-meta-subs.ctd",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Method Properties ¬†¬†¬†",
    "text": "Method Properties ¬†¬†¬†\n\n\n\n\n\n\n\n\nProperty\nMarginal Effect\n\n\n\n\ndata\ntabular\n\n\nfeatures\nnumerical and categorical\n\n\nexplanation\nfeature influence (visualisation)\n\n\ncaveats\nfeature correlation, heterogeneous model response"
  },
  {
    "objectID": "slides/3_feature-based/me.html#computing-me",
    "href": "slides/3_feature-based/me.html#computing-me",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Computing ME",
    "text": "Computing ME\n\n\n\n\n\n\n\nInput\n\n\n\nSelect a feature to explain\nSelect the explanation target\n\ncrisp classifiers ‚Üí one(-vs.-the-rest) or all classes\nprobabilistic classifiers ‚Üí (probabilities of) one class\nregressors ‚Üí numerical values\n\nSelect a collection of instances to generate the explanation"
  },
  {
    "objectID": "slides/3_feature-based/me.html#computing-me-meta-subs.ctd",
    "href": "slides/3_feature-based/me.html#computing-me-meta-subs.ctd",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Computing ME ¬†¬†¬†",
    "text": "Computing ME ¬†¬†¬†\n\n\n\n\n\n\n\nParameters\n\n\n\nIf using the relaxed ME, define binning of the explained feature\n\nnumerical attributes ‚Üí specify (fixed-width or quantile) binning or values of interest with a allowed variation\ncategorical attributes ‚Üí the full set, a subset or grouping of possible values"
  },
  {
    "objectID": "slides/3_feature-based/me.html#computing-me-meta-subs.ctd-1",
    "href": "slides/3_feature-based/me.html#computing-me-meta-subs.ctd-1",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Computing ME ¬†¬†¬†",
    "text": "Computing ME ¬†¬†¬†\n\n\n\n\n\n\n\nProcedure\n\n\n\nIf unavailable, collect predictions of the designated data set\nFor each instance in this set\n\nfor exact ME, assign it to a collection based on its value of the explained feature (possibly multiple instance per value)\nfor relaxed ME, assign it to a bin that spans the range to which the value of its explained feature belongs\n\nGenerate and plot Marginal Effect\n\nfor crisp classifiers count the number of each unique prediction across all the instances collected for every value (exact) or bin (relaxed) of the explained feature; visualise ME either as a count or proportion using separate line for each class or using a stacked bar chart\nfor probabilistic classifiers (per class) and regressors average the response of the model across all the instances collected for each value (exact) or bin (relaxed) of the explained feature; visualise ME as a line\n\n\n¬†¬†¬† Since the values of the explained feature may not be uniformly distributed in the underlying data set, a rug plot showing the distribution of its feature values for strict ME or a histogram representing the number of instances per bin in relaxed ME can help in interpreting the explanation."
  },
  {
    "objectID": "slides/3_feature-based/me.html#formulation-fa-square-root-alt",
    "href": "slides/3_feature-based/me.html#formulation-fa-square-root-alt",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Formulation ¬†¬†¬†",
    "text": "Formulation ¬†¬†¬†\n\\[\nX_{\\mathit{ME}} \\subseteq \\mathcal{X}\n\\]\n\\[\nV_i = \\{ x_i : x \\in X_{\\mathit{ME}} \\}\n\\]\n\\[\n\\mathit{ME}_i =\n\\mathbb{E}_{X_{\\setminus i} | X_{i}} \\left[ f \\left( X_{\\setminus i} , X_{i} \\right) | X_{i}=v_i \\right] =\n\\int_{X_{\\setminus i}} f \\left( X_{\\setminus i} , x_i \\right) \\; d \\mathbb{P} ( X_{\\setminus i} | X_i = v_i )\n\\;\\; \\forall \\; v_i \\in V_i\n\\]\n\n\\[\n\\mathit{ME}_i =\n\\mathbb{E}_{X_{\\setminus i} | X_{i}} \\left[ f \\left( X_{\\setminus i} , X_{i} \\right) | X_{i}=V_i \\right] =\n\\int_{X_{\\setminus i}} f \\left( X_{\\setminus i} , x_i \\right) \\; d \\mathbb{P} ( X_{\\setminus i} | X_i = V_i )\n\\]"
  },
  {
    "objectID": "slides/3_feature-based/me.html#formulation-fa-square-root-alt-meta-subs.ctd",
    "href": "slides/3_feature-based/me.html#formulation-fa-square-root-alt-meta-subs.ctd",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Formulation ¬†¬†¬† ¬†¬†¬†",
    "text": "Formulation ¬†¬†¬† ¬†¬†¬†\n\nBased on the ICE notation (Goldstein et al. 2015)\n\n\\[\n\\left\\{ \\left( x_{S}^{(i)} , x_{C}^{(i)} \\right) \\right\\}_{i=1}^N\n\\]\n\n\\[\n\\hat{f}_S =\n\\mathbb{E}_{X_{C} | X_S} \\left[ \\hat{f} \\left( X_{S} , X_{C} \\right) | X_S = x_S \\right] =\n\\int_{X_C} \\hat{f} \\left( x_{S} , X_{C} \\right) \\; d \\mathbb{P} ( X_{C} | X_S = x_S )\n\\]\n\n\n\\(x_S\\) is fixed ‚Äì the explained feature\n\\(x_C\\) are the given feature values\n\\(X_C\\) and \\(X_S\\) are the random variables\nConditioning the predictions on the distribution of the explained feature(s) yields (average) dependence between the explained feature(s) (including any interactions) and predictions."
  },
  {
    "objectID": "slides/3_feature-based/me.html#approximation-fa-desktop",
    "href": "slides/3_feature-based/me.html#approximation-fa-desktop",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Approximation ¬†¬†¬†",
    "text": "Approximation ¬†¬†¬†\n\n\\[\n\\mathit{ME}_i \\approx\n\\frac{1}{\\sum_{x \\in X_{\\mathit{ME}}} \\mathbb{1} (x_i = v_i)} \\sum_{x \\in X_{\\mathit{ME}}}\nf \\left( x | x_i=v_i \\right)\n\\]"
  },
  {
    "objectID": "slides/3_feature-based/me.html#relaxed-me",
    "href": "slides/3_feature-based/me.html#relaxed-me",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Relaxed ME",
    "text": "Relaxed ME\n\n\nMeasures ME for a range of values \\(v_i \\pm \\delta\\) around a selected value \\(v_i\\), instead of doing so precisely at that point.\n\n\\[\n\\mathit{ME}_i^{\\pm\\delta} =\n\\mathbb{E}_{X_{\\setminus i} | X_{i}} \\left[ f \\left( X_{\\setminus i} , X_{i} \\right) | X_{i}=v_i \\pm \\delta \\right] =\n\\int_{X_{\\setminus i}} f \\left( X_{\\setminus i} , x_i \\right) \\; d \\mathbb{P} ( X_{\\setminus i} | X_i = v_i \\pm \\delta )\n\\;\\; \\forall \\; v_i \\in V_i\n\\]\n\nor\n\n\\[\n\\hat{f}_S^{\\pm\\delta} =\n\\mathbb{E}_{X_{C} | X_S} \\left[ \\hat{f} \\left( X_{S} , X_{C} \\right) | X_S = x_S \\pm \\delta \\right] =\n\\int_{X_C} \\hat{f} \\left( x_{S} , X_{C} \\right) \\; d \\mathbb{P} ( X_{C} | X_S = x_S \\pm \\delta )\n\\]\n\n\nWe can get a more robust measurement of feature influence by relaxing the value for which ME is computed."
  },
  {
    "objectID": "slides/3_feature-based/me.html#strict-me",
    "href": "slides/3_feature-based/me.html#strict-me",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Strict ME",
    "text": "Strict ME"
  },
  {
    "objectID": "slides/3_feature-based/me.html#strict-me-with-standard-deviation",
    "href": "slides/3_feature-based/me.html#strict-me-with-standard-deviation",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Strict ME with Standard Deviation",
    "text": "Strict ME with Standard Deviation"
  },
  {
    "objectID": "slides/3_feature-based/me.html#centred-strict-me-with-standard-deviation",
    "href": "slides/3_feature-based/me.html#centred-strict-me-with-standard-deviation",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Centred Strict ME (with Standard Deviation)",
    "text": "Centred Strict ME (with Standard Deviation)"
  },
  {
    "objectID": "slides/3_feature-based/me.html#strict-me-for-two-numerical-features",
    "href": "slides/3_feature-based/me.html#strict-me-for-two-numerical-features",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Strict ME for Two (Numerical) Features",
    "text": "Strict ME for Two (Numerical) Features"
  },
  {
    "objectID": "slides/3_feature-based/me.html#strict-me-for-crisp-classifiers",
    "href": "slides/3_feature-based/me.html#strict-me-for-crisp-classifiers",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Strict ME for Crisp Classifiers",
    "text": "Strict ME for Crisp Classifiers"
  },
  {
    "objectID": "slides/3_feature-based/me.html#relaxed-me-1",
    "href": "slides/3_feature-based/me.html#relaxed-me-1",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Relaxed ME",
    "text": "Relaxed ME"
  },
  {
    "objectID": "slides/3_feature-based/me.html#relaxed-me-with-standard-deviation",
    "href": "slides/3_feature-based/me.html#relaxed-me-with-standard-deviation",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Relaxed ME with Standard Deviation",
    "text": "Relaxed ME with Standard Deviation"
  },
  {
    "objectID": "slides/3_feature-based/me.html#feature-correlation",
    "href": "slides/3_feature-based/me.html#feature-correlation",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Feature Correlation",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/me.html#feature-correlation-meta-subs.ctd",
    "href": "slides/3_feature-based/me.html#feature-correlation-meta-subs.ctd",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/me.html#feature-correlation-meta-subs.ctd-1",
    "href": "slides/3_feature-based/me.html#feature-correlation-meta-subs.ctd-1",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/me.html#feature-correlation-meta-subs.ctd-2",
    "href": "slides/3_feature-based/me.html#feature-correlation-meta-subs.ctd-2",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation ¬†¬†¬†\n\n\n\nME reports feature influence, but beacause of averaging only similar points the measurement gets distorted by feature correlation, therefore reporting the combined effect\nRemember that we are working with conditional distributions\nIn this example, feature #1 (sepal length) ‚Äì whose coefficient is close to 0 (-0.06) ‚Äì shows strong influence, whcih is due to it being heavily correlated (0.87) with feature #3 (petal length) ‚Äì whose coefficient has the largest magnitude (close to -1.0)\nPD, which is largely immune to this effect, does not display this behaviour"
  },
  {
    "objectID": "slides/3_feature-based/me.html#pros-fa-plus-square",
    "href": "slides/3_feature-based/me.html#pros-fa-plus-square",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Pros ¬†¬†¬†",
    "text": "Pros ¬†¬†¬†\n\nEasy to generate and interpret\nBased on real data"
  },
  {
    "objectID": "slides/3_feature-based/me.html#cons-fa-minus-square",
    "href": "slides/3_feature-based/me.html#cons-fa-minus-square",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Cons ¬†¬†¬†",
    "text": "Cons ¬†¬†¬†\n\nAssumes feature independence, which is often unreasonable and heavily biases the influence measurements\nMay be unreliable for certain values of the explained feature when there is a low number of data points with that value (strict) or in a relevant bin (relaxed); this impacts the reliability of influence estimates (average perdiction of the explained model for that value or range of values)\nReliability of estimates can only be communicated by displaying a rug plot or distribution of instances per value or bin\nDiversity (heterogeneity) of the model‚Äôs behaviour for each particular value or bin can only be communicated by prediction variance\nLimited to explaining two feature at a time"
  },
  {
    "objectID": "slides/3_feature-based/me.html#caveats-fa-skull",
    "href": "slides/3_feature-based/me.html#caveats-fa-skull",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Caveats ¬†¬†¬†",
    "text": "Caveats ¬†¬†¬†\n\nThe measurements may be sensitive to different binning approaches for relaxed ME\nComputational complexity: \\(\\mathcal{O} \\left( n \\right)\\), where \\(n\\) is the number of instances in the designated data set"
  },
  {
    "objectID": "slides/3_feature-based/me.html#related-techniques",
    "href": "slides/3_feature-based/me.html#related-techniques",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Related Techniques",
    "text": "Related Techniques\n\nAccumulated Local Effect (ALE)\n\n ¬†¬†¬† An evolved version of (relaxed) ME that is less prone to being affected by feature correlation. It communicates the influence of a specific feature value on the model‚Äôs prediction by quantifying the average (accumulated) difference between the predictions at the boundaries of a (small) fixed interval around the selected feature value (Apley and Zhu 2020). It is calculated by replacing the value of the explained feature with the interval boundaries for instances found in the designated data set whose value of this feature is within the specified range."
  },
  {
    "objectID": "slides/3_feature-based/me.html#related-techniques-meta-subs.ctd",
    "href": "slides/3_feature-based/me.html#related-techniques-meta-subs.ctd",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Related Techniques ¬†¬†¬†",
    "text": "Related Techniques ¬†¬†¬†\n\nIndividual Conditional Expectation (ICE)\n\n ¬†¬†¬† It communicates the influence of a specific feature value on the model‚Äôs prediction by fixing the value of this feature across a designated range for a selected data point (Goldstein et al. 2015). It is an instance-focused (local) ‚Äúvariant‚Äù of Partial Dependence."
  },
  {
    "objectID": "slides/3_feature-based/me.html#related-techniques-meta-subs.ctd-1",
    "href": "slides/3_feature-based/me.html#related-techniques-meta-subs.ctd-1",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Related Techniques ¬†¬†¬†",
    "text": "Related Techniques ¬†¬†¬†\n\nPartial Dependence (PD)\n\n ¬†¬†¬† It communicates the average influence of a specific feature value on the model‚Äôs prediction by fixing the value of this feature across a designated range for a set of instances. It is a model-focused (global) ‚Äúvariant‚Äù of Individual Conditional Expectation, which is calculated by averaging ICE across a collection of data points (Friedman 2001)."
  },
  {
    "objectID": "slides/3_feature-based/me.html#implementations",
    "href": "slides/3_feature-based/me.html#implementations",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Implementations",
    "text": "Implementations\n\n\n\n\n\n\n\n Python\n R\n\n\n\n\nN/A\nDALEX"
  },
  {
    "objectID": "slides/3_feature-based/me.html#further-reading",
    "href": "slides/3_feature-based/me.html#further-reading",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Further Reading",
    "text": "Further Reading\n\nInterpretable Machine Learning book\nExplanatory Model Analysis book"
  },
  {
    "objectID": "slides/3_feature-based/me.html#bibliography",
    "href": "slides/3_feature-based/me.html#bibliography",
    "title": "Marginal Effect (ME)/Marginal Plots, M-Plots orLocal Dependence Profiles/",
    "section": "Bibliography",
    "text": "Bibliography\n\n\nApley, Daniel W, and Jingyu Zhu. 2020. ‚ÄúVisualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.‚Äù Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82 (4): 1059‚Äì86.\n\n\nFriedman, Jerome H. 2001. ‚ÄúGreedy Function Approximation: A Gradient Boosting Machine.‚Äù Annals of Statistics, 1189‚Äì1232.\n\n\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. ‚ÄúPeeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.‚Äù Journal of Computational and Graphical Statistics 24 (1): 44‚Äì65."
  },
  {
    "objectID": "slides/3_feature-based/ale.html#explanation-synopsis",
    "href": "slides/3_feature-based/ale.html#explanation-synopsis",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Explanation Synopsis",
    "text": "Explanation Synopsis\n\n\nALE captures the influence of a specific feature value on the model‚Äôs prediction by quantifying the average (accumulated) difference between the predictions at the boundaries of a (small) fixed interval around the selected feature value (Apley and Zhu 2020). It is calculated by replacing the value of the explained feature with the interval boundaries for instances found in the designated data set whose value of this feature is within the specified range.\n\n\n\nIt communicates global (with respect to the entire explained model) feature influence."
  },
  {
    "objectID": "slides/3_feature-based/ale.html#rationale",
    "href": "slides/3_feature-based/ale.html#rationale",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Rationale",
    "text": "Rationale\n\n\nALE is an evolved version of (relaxed)  Marginal Effect (ME) (Apley and Zhu 2020) that is less prone to being affected by feature correlation since it relies upon average prediction change. It also improves upon  Partial Dependence (PD) (Friedman 2001) by ensuring that the influence estimates are based on realistic instances (thus respecting interactions between features / feature correlation), making the explanatory insights more truthful."
  },
  {
    "objectID": "slides/3_feature-based/ale.html#toy-example-numerical-feature",
    "href": "slides/3_feature-based/ale.html#toy-example-numerical-feature",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Toy Example ‚Äì Numerical Feature",
    "text": "Toy Example ‚Äì Numerical Feature\n\n\n\nALE values is the change in the output of the predictive model ‚Äì probability of a particular class here ‚Äì within feature partitions over the range of the feature, in reference to the average prediction\nALE values can be interpreted as the (main) effect of the feature at a certain value (middle of the bin) compared to its average effect; e.g., the estimate of +0.15 at petal length (cm) 2.75, communicates that for this particular feature value the effect is higher by 0.15 than the average effect of -0.59"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#method-properties",
    "href": "slides/3_feature-based/ale.html#method-properties",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Method Properties",
    "text": "Method Properties\n\n\n\n\n\n\n\n\nProperty\nAccumulated Local Effect\n\n\n\n\nrelation\npost-hoc\n\n\ncompatibility\nmodel-agnostic\n\n\nmodelling\nregression and probabilistic classification (numbers)\n\n\nscope\nglobal (per data set; generalises to cohort)\n\n\ntarget\nmodel (set of predictions)\n\n\n\n\n\nBecause of the difference in prediction at bin boundaries, ALE does not work with crisp classifiers"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#method-properties-meta-subs.ctd",
    "href": "slides/3_feature-based/ale.html#method-properties-meta-subs.ctd",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Method Properties ¬†¬†¬†",
    "text": "Method Properties ¬†¬†¬†\n\n\n\n\n\n\n\n\nProperty\nAccumulated Local Effect\n\n\n\n\ndata\ntabular\n\n\nfeatures\nnumerical (ordinal categorical)\n\n\nexplanation\nfeature influence (visualisation)\n\n\ncaveats\nfeature binning\n\n\n\n\n\nBecause of pushing instances to bin boundaries created for the explained feature ‚Äì to calculate the difference in prediction at these two points ‚Äì ALE does not work with categorical features out of the box\nThis limitation can be overcome by establishing an order among categorical features\nFor example, the order can be induced by comparing similarity between categories based on other feature values"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale",
    "href": "slides/3_feature-based/ale.html#computing-ale",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE",
    "text": "Computing ALE\n\n\n\n\n\n\n\nInput\n\n\n\nSelect a feature to explain\nSelect the explanation target\n\nprobabilistic classifiers ‚Üí (probabilities of) one class\nregressors ‚Üí numerical values\n\nSelect a collection of instances to generate the explanation"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE ¬†¬†¬†\n\n\n\n\n\n\n\nParameters\n\n\n\nDefine binning of the explained (numerical) feature\n\nselect the number of bins\ndecide on fixed-width, quantile or custom binning"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-1",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-1",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE ¬†¬†¬†\n\n\n\n\n\n\n\nProcedure\n\n\n\nFor each instance in the designated data set, assign it to a bin that spans the range to which the value of its explained feature belongs"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-2",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-2",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-3",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-3",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE ¬†¬†¬†\n\n\n\n\n\n\n\nProcedure ¬†¬†¬†\n\n\n\nFor each instance in each bin, calculate the difference between the prediction of these instances at bin boundaries"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-4",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-4",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-5",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-5",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE ¬†¬†¬†\n\n\n\n\n\n\n\nProcedure ¬†¬†¬†\n\n\n\nCalculate the mean change in prediction for each bin"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-6",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-6",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-7",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-7",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE ¬†¬†¬†\n\n\n\n\n\n\n\nProcedure ¬†¬†¬†\n\n\n\nAccumulate the mean change in prediction over the bins"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-8",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-8",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-9",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-9",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-10",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-10",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE ¬†¬†¬†\n\n\n\n\n\n\n\nProcedure ¬†¬†¬†\n\n\n\nExtrapolate the value of the accumulated mean change in prediction in the middle of each bin"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-11",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-11",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-12",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-12",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE ¬†¬†¬†\n\n\n\n\n\n\n\nProcedure ¬†¬†¬†\n\n\n\nCentre (the extrapolated value of) the accumulated mean change in prediction in the middle of each bin around their mean\n\n¬†¬†¬† Depending on the binning strategy, the number of instances per bin may be distributed unevenly. A histogram representing the number of instances in each bin can help in interpreting the explanation.\n\n\n\n\n\nQuantile binning offers even distribution of instances per bin, but may result in bins of disparate lengths\nOther binning may result in underrepresented bins"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-13",
    "href": "slides/3_feature-based/ale.html#computing-ale-meta-subs.ctd-13",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Computing ALE ¬†¬†¬†",
    "text": "Computing ALE"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#formulation-fa-square-root-alt",
    "href": "slides/3_feature-based/ale.html#formulation-fa-square-root-alt",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Formulation ¬†¬†¬†",
    "text": "Formulation ¬†¬†¬†\n\\[\nX_{\\mathit{ALE}} \\subseteq \\mathcal{X}\n\\]\n\\[\nV_i = \\{ x_i : x \\in X_{\\mathit{ALE}} \\}\n\\]\n\\[\n\\mathit{ALE}_i =\n\\int_{v_{0}}^{x_i}\n\\mathbb{E}_{X_{\\setminus i} | X_{i}=x_i} \\left[ f^i \\left( X_{\\setminus i} , X_{i} \\right) | X_{i}=v_i \\right]\n\\; d v_i\n- \\mathit{const}\n\\\\\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=\n\\int_{v_{0}}^{x_i} \\left (\n\\int_{X_{\\setminus i}} f^i \\left( X_{\\setminus i} , v_i \\right) \\; d \\mathbb{P} ( X_{\\setminus i} | X_i = v_i )\n\\right )\n\\; d v_i\n- \\mathit{const}\n\\]\n\n\\[\nf^i (x_{\\setminus i}, x_i) = \\frac{\\partial f (x_{\\setminus i}, x_i)}{\\partial x_i}\n\\]\n\n\nThere are 2 differences between this formulation and ME formulation\n\n\\(f\\) is replaced with \\(f^i\\) to reflect that we are interested in the (average [expected]) change of prediction when \\(x_i\\) changes\nthe outer integral \\(\\int_{v_{0}}^{x_i}\\) captures the accumulation over values \\(v_0, \\ldots x_i\\) ‚Äì from minimum up to the value for that feature of the explained instace ‚Äì for the feature \\(x_i\\) (in computation these become discrete intervals)\n\nThe outer integral and the partial derivative in conjunction ensure that we isolate the effect of the explained feature from other features\nBy subtracting a constant (\\(\\mathit{const}\\)) we centre ALE such that the average effect is 0\nConditioning the difference in predictions on the distribution of the explained feature(s) yields (average) effect of the explained feature(s) (and this feature alone) on predictions"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#formulation-fa-square-root-alt-meta-subs.ctd",
    "href": "slides/3_feature-based/ale.html#formulation-fa-square-root-alt-meta-subs.ctd",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Formulation ¬†¬†¬† ¬†¬†¬†",
    "text": "Formulation ¬†¬†¬† ¬†¬†¬†\n\nBased on the ICE notation (Goldstein et al. 2015)\n\n\\[\n\\hat{f}_S =\n\\int_{z_{0, S}}^{x_S}\n\\mathbb{E}_{X_{C} | X_S = x_S} \\left[ \\hat{f}^{S} \\left( X_{S} , X_{C} \\right) | X_S = z_S \\right] \\; d z_{S} - \\mathit{const} \\\\\n\\;\\;\\;\\;\\;\\;\\;\\;=\n\\int_{z_{0, S}}^{x_S} \\left (\n\\int_{X_C} \\hat{f}^{S} \\left( z_{S} , X_{C} \\right) \\; d \\mathbb{P} ( X_{C} | X_S = z_S )\n\\right )\n\\; d z_{S} - \\mathit{const}\n\\]\n\n\\[\n\\hat{f}^{S} (x_s, x_c) = \\frac{\\partial \\hat{f} (x_S, x_C)}{\\partial x_S}\n\\]\n\n\nThere are 2 differences between this formulation and ME formulation\n\n\\(\\hat{f}\\) is replaced with \\(\\hat{f}^{S}\\) to reflect that we are interested in the (average [expected]) change of prediction when \\(x_S\\) changes\nthe outer integral \\(\\int_{z_{0, S}}^{x_S}\\) captures the accumulation over values \\(z_0, \\ldots x_S\\) ‚Äì from minimum up to the value for that feature of the explained instace ‚Äì for the feature \\(X_S\\) (in computation these become discrete intervals)\n\nThe outer integral and the partial derivative in conjunction ensure that we isolate the effect of the explained feature from other features\nBy subtracting a constant (\\(\\mathit{const}\\)) we centre ALE such that the average effect is 0\n\\(x_S\\) is fixed ‚Äì the explained feature\n\\(x_C\\) are the given feature values\n\\(X_C\\) and \\(X_S\\) are the random variables\nConditioning the difference in predictions on the distribution of the explained feature(s) yields (average) effect of the explained feature(s) (and this feature alone) on predictions"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#approximation-fa-desktop",
    "href": "slides/3_feature-based/ale.html#approximation-fa-desktop",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Approximation ¬†¬†¬†",
    "text": "Approximation ¬†¬†¬†\n\n\\[\n\\mathit{ALE}_i^{j} \\approx\n\\sum_{n=1}^{j}\n\\frac{1}{|Z_n|}\n\\sum_{x \\in Z_n}\n\\left[\nf \\left( x_{\\setminus i} , x_i=Z_n^+ \\right) -\nf \\left( x_{\\setminus i} , x_i=Z_n^- \\right)\n\\right]\n\\]\n\n\\[\n\\overline{\\mathit{ALE}_i^{j}} =\n\\mathit{ALE}_i^{j} -\n\\frac{1}{\\sum_{Z_n \\in Z} |Z_n|}\n\\sum_{x \\in Z}\n\\mathit{ALE}_i(x)\n\\]\n\n\nThe top one is uncentred; the bottom one is centred (no \\(j\\) superscript in \\(\\mathit{ALE}_i\\) means that we go to the interval where the instance \\(x\\) is located)\n\\(\\mathit{ALE}_i^j\\) ‚Äì ALE of feature \\(i\\) for interval \\(j\\)\n\\(Z_n\\) is the \\(n\\)-th interval; \\(Z_n^-\\) is the lower bound of the \\(n\\)-th interval; and \\(Z_n^+\\) is the upper bound of the \\(n\\)-th interval\nSince we may not have access to gradient ([partial] derivative) of the predictive function, we use difference over intervals (approximation)\neffect is the difference in prediction in a given interval, which makes it local; we accumulate this effect up to inteval \\(j\\)"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-binning-approaches",
    "href": "slides/3_feature-based/ale.html#feature-binning-approaches",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Binning Approaches",
    "text": "Feature Binning Approaches\n\n\nGiven the need for binning, various approaches such as:\n\nquantile,\nequal-width or\ncustom.\n\ncan be used.\n(Examples to follow.)"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#multi-dimensional-ale",
    "href": "slides/3_feature-based/ale.html#multi-dimensional-ale",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Multi-dimensional ALE",
    "text": "Multi-dimensional ALE\n\n\nALE of a single feature captures only the effect of this particular feature on the explained model‚Äôs predictive behaviour ‚Äì known as first-order effect. ALE of multiple features capture the exclusive effect of the interaction between n features on the explained model‚Äôs predictive behaviour (adjusted for the overall effect as well as the main effect of each feature) ‚Äì known as nth-order effect, e.g., second-order effect.\n(Examples to follow.)\n\n\n\n\n\n\n\n\nFormulation ¬†¬†¬†\n\n\nRefer to Apley and Zhu (2020) for the formulation."
  },
  {
    "objectID": "slides/3_feature-based/ale.html#multi-dimensional-ale-meta-subs.ctd",
    "href": "slides/3_feature-based/ale.html#multi-dimensional-ale-meta-subs.ctd",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Multi-dimensional ALE ¬†¬†¬†",
    "text": "Multi-dimensional ALE ¬†¬†¬†\n\n\n\n\n\n\n\nComputation ¬†¬†¬†\n\n\n\\[\n\\underbrace{\n\\overbrace{(n - m)}^{\\text{feature #1}}\n-\n\\overbrace{(b - a)}^{\\text{feature #1}}\n}_{\\text{feature #2}}\n\\]"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#ale",
    "href": "slides/3_feature-based/ale.html#ale",
    "title": "Accumulated Local Effect (ALE)",
    "section": "ALE",
    "text": "ALE"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#ale-with-standard-deviation",
    "href": "slides/3_feature-based/ale.html#ale-with-standard-deviation",
    "title": "Accumulated Local Effect (ALE)",
    "section": "ALE with Standard Deviation",
    "text": "ALE with Standard Deviation"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#uniform-binning-ale-with-standard-deviation",
    "href": "slides/3_feature-based/ale.html#uniform-binning-ale-with-standard-deviation",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Uniform Binning ALE (with Standard Deviation)",
    "text": "Uniform Binning ALE (with Standard Deviation)"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#ale-for-two-features",
    "href": "slides/3_feature-based/ale.html#ale-for-two-features",
    "title": "Accumulated Local Effect (ALE)",
    "section": "ALE for Two Features",
    "text": "ALE for Two Features"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation",
    "href": "slides/3_feature-based/ale.html#feature-correlation",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation",
    "text": "Feature Correlation\n\n\n\nIf a feature has not effect on the prediction (in a given section of a feature), it shows in ALE as a straight line\nALE is centred around 0 if it has no effect on the feature throughout the entire range\nSince the underlying model is linear, features are assumed to be independent and have linear effect on the prediction"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-1",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-1",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation ¬†¬†¬†\n\n\n\nThis agrees with the insights communicated by ALE ‚Äì despite diverse predictions, indiviudal response is quite stable\nNote: By design, PD reports the total (all, up to the nth-order effects), but ALE separates effects of a given order"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-2",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-2",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-3",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-3",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation ¬†¬†¬†\n\n\n\n\n\n\n\nALE and Linear Model Coefficients\n\n\nSee Gr√∂mping (2020) for an explanation why ALE may not reflect the coefficients of a linear model.\n\n\n\n\n\nBut based on the model coefficients, we can see that other features contribute to the prediction"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-4",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-4",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation ¬†¬†¬†\n\n\n\nThese somewhat counterintuitive explanations are caused by strong feature correlation"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-5",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-5",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation ¬†¬†¬†\n\n\n\nRecall: Second-order effect communicates the additional effect due to interaction between two features on the predictions of the explained model, after accoutning for the main effect of both features\nMost of second-order effects are close to 0"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-6",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-6",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-7",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-7",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-8",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-8",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation ¬†¬†¬†\n\n\n\nThe negative effect of this second-order ALE is non-negligible (albeit in a small region of the feature grid)\nWhile the undelrying model is linear and presupposes no feature interaction, the feature correlation cannot be ignored\nWhile petal length (cm) is the main predictive feature, its interaction with sepal width (cm) in a small region helps to predict the classes\npetal length (cm) ‚Äì y-axis ‚Äì is influential in first-order ALE and strongly correlated (mostly positive) with\n\nsepal length (cm) (+0.87)\npetal width (cm) (+0.96)\nsepal width (cm) (-0.43)\n\nsepal width (cm) ‚Äì x-axis ‚Äì is weakly correlated (negative) with\n\nsepal length (cm) (-0.12)\npetal width (cm) (-0.43)\nsepal length (cm) (-0.37)"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-9",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-9",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-10",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-10",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-11",
    "href": "slides/3_feature-based/ale.html#feature-correlation-meta-subs.ctd-11",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Feature Correlation ¬†¬†¬†",
    "text": "Feature Correlation"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#pros-fa-plus-square",
    "href": "slides/3_feature-based/ale.html#pros-fa-plus-square",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Pros ¬†¬†¬†",
    "text": "Pros ¬†¬†¬†\n\nEasy and fast to generate\nReasonably easy to interpret (first-order ALE)\nReliable when features are correlated (unbiased)\nBased on data that are closely distributed to the real data"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#cons-fa-minus-square",
    "href": "slides/3_feature-based/ale.html#cons-fa-minus-square",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Cons ¬†¬†¬†",
    "text": "Cons ¬†¬†¬†\n\nNot so easy to implement\nTricky to interpret for orders higher than first\nLimited to explaining two feature at a time\nALE trends should not be generalised to individual instances across the feature range since the estimates are specific to each bin"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#cons-fa-minus-square-meta-subs.ctd",
    "href": "slides/3_feature-based/ale.html#cons-fa-minus-square-meta-subs.ctd",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Cons ¬†¬†¬† ¬†¬†¬†",
    "text": "Cons ¬†¬†¬† ¬†¬†¬†\n\nBinning may skew the results (aided by displaying distribution of instances per bin); e.g.,\n\nquantiles ensure good estimates given the number of instances per bin, but may yield unusually long and short bins;\nfixed-width offers regular bins, but some may lack a sufficient number of points to offer reliable estimates"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#caveats-fa-skull",
    "href": "slides/3_feature-based/ale.html#caveats-fa-skull",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Caveats ¬†¬†¬†",
    "text": "Caveats ¬†¬†¬†\n\nThe measurements may be sensitive to different binning approaches\nComputational complexity: \\(\\mathcal{O} \\left( n \\right)\\), where \\(n\\) is the number of instances in the designated data set"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#related-techniques",
    "href": "slides/3_feature-based/ale.html#related-techniques",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Related Techniques",
    "text": "Related Techniques\n\nMarginal Effect (ME)\n\n ¬†¬†¬† ME captures the average response of a predictive model across a collection of instances (taken from a designated data set) for a specific value of a selected feature (found in the aforementioned data set) (Apley and Zhu 2020). When relaxed by including similar feature values determined by a fixed interval around the selected value, this method offers similar insights to ALE: average prediction per interval instead of (accumulated) difference in prediction per interval."
  },
  {
    "objectID": "slides/3_feature-based/ale.html#related-techniques-meta-subs.ctd",
    "href": "slides/3_feature-based/ale.html#related-techniques-meta-subs.ctd",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Related Techniques ¬†¬†¬†",
    "text": "Related Techniques ¬†¬†¬†\n\nIndividual Conditional Expectation (ICE)\n\n ¬†¬†¬† It communicates the influence of a specific feature value on the model‚Äôs prediction by fixing the value of this feature across a designated range for a selected data point (Goldstein et al. 2015). It is an instance-focused (local) ‚Äúvariant‚Äù of Partial Dependence."
  },
  {
    "objectID": "slides/3_feature-based/ale.html#related-techniques-meta-subs.ctd-1",
    "href": "slides/3_feature-based/ale.html#related-techniques-meta-subs.ctd-1",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Related Techniques ¬†¬†¬†",
    "text": "Related Techniques ¬†¬†¬†\n\nPartial Dependence (PD)\n\n ¬†¬†¬† It communicates the average influence of a specific feature value on the model‚Äôs prediction by fixing the value of this feature across a designated range for a set of instances. It is a model-focused (global) ‚Äúvariant‚Äù of Individual Conditional Expectation, which is calculated by averaging ICE across a collection of data points (Friedman 2001)."
  },
  {
    "objectID": "slides/3_feature-based/ale.html#implementations",
    "href": "slides/3_feature-based/ale.html#implementations",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Implementations",
    "text": "Implementations\n\n\n\n\n\n\n\n Python\n R\n\n\n\n\nALEPython\nALEPlot\n\n\nalibi\nDALEX\n\n\n\niml"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#further-reading",
    "href": "slides/3_feature-based/ale.html#further-reading",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Further Reading",
    "text": "Further Reading\n\nALE paper (Apley and Zhu 2020)\nInterpretable Machine Learning book\nExplanatory Model Analysis book"
  },
  {
    "objectID": "slides/3_feature-based/ale.html#bibliography",
    "href": "slides/3_feature-based/ale.html#bibliography",
    "title": "Accumulated Local Effect (ALE)",
    "section": "Bibliography",
    "text": "Bibliography\n\n\nApley, Daniel W, and Jingyu Zhu. 2020. ‚ÄúVisualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.‚Äù Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82 (4): 1059‚Äì86.\n\n\nFriedman, Jerome H. 2001. ‚ÄúGreedy Function Approximation: A Gradient Boosting Machine.‚Äù Annals of Statistics, 1189‚Äì1232.\n\n\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. ‚ÄúPeeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.‚Äù Journal of Computational and Graphical Statistics 24 (1): 44‚Äì65.\n\n\nGr√∂mping, Ulrike. 2020. ‚ÄúModel-Agnostic Effects Plots for Interpreting Machine Learning Models.‚Äù Reports in Mathematics, Physics and Chemistry, Department II, Beuth University of Applied Sciences Berlin Report 1."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#explanation-synopsis",
    "href": "slides/3_feature-based/pfi.html#explanation-synopsis",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Explanation Synopsis",
    "text": "Explanation Synopsis\n\n\nPFI ‚Äì sometimes called Model Reliance (Fisher, Rudin, and Dominici 2019) ‚Äì quantifies importance of a feature by measuring the change in predictive error incurred when permuting its values for a collection of instances (Breiman 2001).\n\n\n\nIt communicates global (with respect to the entire explained model) feature importance."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#rationale",
    "href": "slides/3_feature-based/pfi.html#rationale",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Rationale",
    "text": "Rationale\n\n\nPFI was originally introduced for  Random Forests (Breiman 2001) and later generalised to a model-agnostic technique under the name of Model Reliance (Fisher, Rudin, and Dominici 2019)."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#toy-example",
    "href": "slides/3_feature-based/pfi.html#toy-example",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Toy Example",
    "text": "Toy Example\n\n\n\nYou could use box plots or violin plots as an alternative visualisation technique."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#method-properties",
    "href": "slides/3_feature-based/pfi.html#method-properties",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Method Properties",
    "text": "Method Properties\n\n\n\n\n\n\n\n\nProperty\nPermutation Feature Importance\n\n\n\n\nrelation\npost-hoc\n\n\ncompatibility\nmodel-agnostic\n\n\nmodelling\nregression, crisp and probabilistic classification\n\n\nscope\nglobal (per data set; generalises to cohort)\n\n\ntarget\nmodel (set of predictions)"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#method-properties-meta-subs.ctd",
    "href": "slides/3_feature-based/pfi.html#method-properties-meta-subs.ctd",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Method Properties ¬†¬†¬†",
    "text": "Method Properties ¬†¬†¬†\n\n\n\n\n\n\n\n\nProperty\nPermutation Feature Importance\n\n\n\n\ndata\ntabular\n\n\nfeatures\nnumerical and categorical\n\n\nexplanation\nfeature importance (numerical reporting, visualisation)\n\n\ncaveats\nfeature correlation, model‚Äôs goodness of fit, access to data labels, robustness (randomness of permutation)"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#computing-pfi",
    "href": "slides/3_feature-based/pfi.html#computing-pfi",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Computing PFI",
    "text": "Computing PFI\n\n\n\n\n\n\n\nInput\n\n\n\nOptionally, select a subset of features to explain\nSelect a predictive performance metric to assess degradation of utility when permuting the features; it has to be compatible with the type of the modelling problem (crisp classification, probabilistic classification or regression)\nSelect a collection of instances to generate the explanation"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#computing-pfi-meta-subs.ctd",
    "href": "slides/3_feature-based/pfi.html#computing-pfi-meta-subs.ctd",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Computing PFI ¬†¬†¬†",
    "text": "Computing PFI ¬†¬†¬†\n\n\n\n\n\n\n\nParameters\n\n\n\nDefine the number of rounds during which feature values will be permuted and the drop in performance recorded\nSpecify the permutation protocol\n\n\n\n\n \n\n\n\n\n\n\nPermutation Protocol\n\n\nPFI is limited to tabular data primarily due to he nature of the employed feature permutation protocol. In theory, this limitation can be overcome and PFI expanded to other data types if a meaningful permutation strategy ‚Äì or a suitable proxy ‚Äì can be designed."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#computing-pfi-meta-subs.ctd-1",
    "href": "slides/3_feature-based/pfi.html#computing-pfi-meta-subs.ctd-1",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Computing PFI ¬†¬†¬†",
    "text": "Computing PFI ¬†¬†¬†\n\n\n\n\n\n\n\nProcedure\n\n\n\nCalculate predictive performance of the explained model on the provided data using the designated metric\nFor each feature selected to be explained, permute its values\n\nEvaluate performance of the explained model on the altered data set\nQuantify the change in predictive performance\n\nRepeat the process the number of times specified by the user to improve the reliability of the importance estimate"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#formulation-fa-square-root-alt",
    "href": "slides/3_feature-based/pfi.html#formulation-fa-square-root-alt",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Formulation ¬†¬†¬†",
    "text": "Formulation ¬†¬†¬†\n\n\\[\nI_{\\textit{PFI}}^{j} =\n  \\frac{1}{N} \\sum_{i = 1}^N\n    \\frac{\\overbrace{\\mathcal{L}(f(X^{(j)}), Y)}^{\\text{permute feature j}}}{\\mathcal{L}(f(X), Y)}\n\\]\n\n\n\\(N\\) ‚Äì number of runs"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#performance-change-quantification-fa-desktop",
    "href": "slides/3_feature-based/pfi.html#performance-change-quantification-fa-desktop",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Performance Change Quantification ¬†¬†¬†",
    "text": "Performance Change Quantification ¬†¬†¬†\n\n\n\n\nDifference \\[\n\\mathcal{L}(f(X^{(j)}), Y)\n-\n\\mathcal{L}(f(X), Y)\n\\]\nQuotient \\[\n\\frac{\\mathcal{L}(f(X^{(j)}), Y)}{\\mathcal{L}(f(X), Y)}\n\\]\n\n\n\nPercent change  \\[\n100 \\times \\frac{\\mathcal{L}(f(X^{(j)}), Y) - \\mathcal{L}(f(X), Y)}{\\mathcal{L}(f(X), Y)}\n\\]\n\n\n\n\n\nUsing the quotient (ratio) or percentage change makes the metric comparable across different problems (in contrast to the difference, which is not invariant to scale)"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#selecting-data",
    "href": "slides/3_feature-based/pfi.html#selecting-data",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Selecting Data",
    "text": "Selecting Data\n\n\nPFI needs a representative sample of data to output a meaningful explanation\nThe meaning of PFI is decided by the sample of data used for its generation\nSome choices are\n\nTraining Data ‚Äì instances used to train the explained model\nValidation Data ‚Äì instances used to evaluate the predictive performance the explained model; also employed for hyperparameter tuning\nTest Data ‚Äì instances used to estimate the final, unbiased predictive performance of the explained model\nExplainability Data ‚Äì a separate pool of instances reserved for explaining the behaviour of the model"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#pfi-based-on-training-data",
    "href": "slides/3_feature-based/pfi.html#pfi-based-on-training-data",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "PFI Based on Training Data",
    "text": "PFI Based on Training Data\n\n\nThis explanation communicates how the model relies on data features during training, but not necessarily how the features influence predictions of unseen instances. The model may learn a relationship between a feature and the target variable that is due to a quirk of the training data ‚Äì a random pattern present only in the training data sample that, e.g., due to overfitting, can add some extra performance just for predicting the training data."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#pfi-based-on-training-data-meta-subs.ctd",
    "href": "slides/3_feature-based/pfi.html#pfi-based-on-training-data-meta-subs.ctd",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "PFI Based on Training Data ¬†¬†¬†",
    "text": "PFI Based on Training Data ¬†¬†¬†\n\n\n\nHere we train a model on 2 features of the Iris data set ‚Äì sepal length (cm) and sepal width (cm) ‚Äì expanded with 1 random feature\nThe model has learnt to rely on this random feature in conjunction with the real features possibly due to spurious correlations between these attributes found in the training data"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#pfi-based-on-test-data",
    "href": "slides/3_feature-based/pfi.html#pfi-based-on-test-data",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "PFI Based on Test Data",
    "text": "PFI Based on Test Data\n\n\nThe spurious correlations between data features and the target found uniquely in the training data or extracted due to overfitting are absent in the test data (previously unseen by the model). This allows PFI to communicate how useful each feature is for predicting the target, or whether some of the data feature contributed to overfitting."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#pfi-based-on-test-data-meta-subs.ctd",
    "href": "slides/3_feature-based/pfi.html#pfi-based-on-test-data-meta-subs.ctd",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "PFI Based on Test Data ¬†¬†¬†",
    "text": "PFI Based on Test Data ¬†¬†¬†\n\n\n\nWhen the spurious pattern is broken the predictive performance may show improvement on permuted instances.\nNegative PFI indicates this behaviour."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance",
    "href": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Other Measures of Feature Importance",
    "text": "Other Measures of Feature Importance\n\nPD-based Feature Importance\n\n\nWe can measure feature importance with alternative techniques such as  Partial Dependence-based feature importance. This metric may not pick up the random feature‚Äôs lack of predictive power since PD generates unrealistic instances that could follow the spurious pattern found in the training data."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance-meta-subs.ctd",
    "href": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance-meta-subs.ctd",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Other Measures of Feature Importance ¬†¬†¬†",
    "text": "Other Measures of Feature Importance ¬†¬†¬†\n\nPD-based Feature Importance ¬†¬†¬†\n\n\n\nThe advantage of this technique is that we can have per-class feature importance estimates given the nature of PD."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance-meta-subs.ctd-1",
    "href": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance-meta-subs.ctd-1",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Other Measures of Feature Importance ¬†¬†¬†",
    "text": "Other Measures of Feature Importance ¬†¬†¬†\n\nPD-based Feature Importance"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance-meta-subs.ctd-2",
    "href": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance-meta-subs.ctd-2",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Other Measures of Feature Importance ¬†¬†¬†",
    "text": "Other Measures of Feature Importance ¬†¬†¬†\n\nPD-based Feature Importance"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance-meta-subs.ctd-3",
    "href": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance-meta-subs.ctd-3",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Other Measures of Feature Importance ¬†¬†¬†",
    "text": "Other Measures of Feature Importance ¬†¬†¬†\n\nTree-based Feature Importance\n\nSince the underlying predictive model (the one being explained) is a  Decision Tree, we have access to its native estimate of feature importance. It conveys the overall decrease in the chosen impurity metric for all splits based on a given feature, by default calculated over the training data.\n\n \n\n\n\n\n\n\nEstimate Based on Alternative Data Set\n\n\nConsider implementing the same feature importance calculation protocol for other data sets, e.g., the test data."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance-meta-subs.ctd-4",
    "href": "slides/3_feature-based/pfi.html#other-measures-of-feature-importance-meta-subs.ctd-4",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Other Measures of Feature Importance ¬†¬†¬†",
    "text": "Other Measures of Feature Importance ¬†¬†¬†\nTree-based Feature Importance ¬†¬†¬†\n\n\n\nSince this measurement is also based on training data, the random feature is considered important."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#pfi-bar-plot",
    "href": "slides/3_feature-based/pfi.html#pfi-bar-plot",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "PFI ‚Äì Bar Plot",
    "text": "PFI ‚Äì Bar Plot\n\n\n\nThe error bar ‚Äì the black vertical line attache to the top of each PFI bar ‚Äì communicates the standard deviation of calculating PFI over multiple runs."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#pfi-box-plot",
    "href": "slides/3_feature-based/pfi.html#pfi-box-plot",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "PFI ‚Äì Box Plot",
    "text": "PFI ‚Äì Box Plot\n\n\n\nThe data are based on multiple runs of PFI calculation with different permutation of the explained feature\nThe box plots show:\n\ngreen line ‚Äì median\nbox ‚Äì stretches between lower and upper quartiles\nwhiskers ‚Äì span the range of data\nflier points ‚Äì none shown\n\nAdditionally, the plot visualises:\n\nred triangle ‚Äì mean\nblue points ‚Äì individual PFI scores"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#pfi-violin-plot",
    "href": "slides/3_feature-based/pfi.html#pfi-violin-plot",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "PFI ‚Äì Violin Plot",
    "text": "PFI ‚Äì Violin Plot\n\n\n\nThe data are based on multiple runs of PFI calculation with different permutation of the explained feature\nThe violin plots show the distribution of importance scores for each feature (density estimated using a Gaussian kernel)\nThis is supplemented by:\n\ngreen points ‚Äì individual PFI scores\nblue cross ‚Äì mean"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#pfi-for-different-metrics",
    "href": "slides/3_feature-based/pfi.html#pfi-for-different-metrics",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "PFI for Different Metrics",
    "text": "PFI for Different Metrics\n\n\n\nThe Importance score range and proportion is influenced by the selected predictive performance metric"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#out-of-distribution-impossible-instances",
    "href": "slides/3_feature-based/pfi.html#out-of-distribution-impossible-instances",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Out-of-distribution (Impossible) Instances",
    "text": "Out-of-distribution (Impossible) Instances\n\n\n\nPermutation results in out-of-distribution instances, therefore PFI may not be reliable"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#feature-correlation",
    "href": "slides/3_feature-based/pfi.html#feature-correlation",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Feature Correlation",
    "text": "Feature Correlation\n\n\n\nIn the case of the Iris data set, the out-of-distribution instances are predominantly caused by strong feature correlation"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#pros-fa-plus-square",
    "href": "slides/3_feature-based/pfi.html#pros-fa-plus-square",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Pros ¬†¬†¬†",
    "text": "Pros ¬†¬†¬†\n\nEasy to generate and interpret\nAll of the features can be explained at the same time\nComputationally efficient in comparison to a brute-force approach such as leave-one-out and retrain (which also has a different interpretation)\nAccounts for the importance of the explained feature and all of its interactions with other features (which can also be considered a disadvantage)"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#cons-fa-minus-square",
    "href": "slides/3_feature-based/pfi.html#cons-fa-minus-square",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Cons ¬†¬†¬†",
    "text": "Cons ¬†¬†¬†\n\nRequires access to ground truth (i.e., data and their labels)\nInfluenced by randomness of permuting feature values (somewhat abated by repeating the calculation mulitple times at the expense of extra compute)\nRelies on the underlying model‚Äôs goodness of fit since it is based on (the drop in) a predictive perfromance metric (in contrast to a more generic change in predictive behaviour ‚Äì think predictive robustness)"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#cons-fa-minus-square-meta-subs.ctd",
    "href": "slides/3_feature-based/pfi.html#cons-fa-minus-square-meta-subs.ctd",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Cons ¬†¬†¬† ¬†¬†¬†",
    "text": "Cons ¬†¬†¬† ¬†¬†¬†\n\nAssumes feature independence, which is often unreasonable\nMay not reflect the true feature importance since it is based upon the predictive ability of the model for unrealistic instances\nIn presence of feature interaction, the importance ‚Äì that one of the attributes would accumulate if alone ‚Äì may be distributed across all of them in an arbitrary fashion (pushing them down the order of importance)\nSince it accounts for indiviudal and interaction importance, the latter component is accounted for multiple times, making the sum of the scores inconsistent with (larger than) the drop in predictive performance (for the difference-based variant)\n\n\n\nCaveat: The importance distribution only reflects how the model perceives the importance of feature (i.e., its behaviour) and not their true importance (i.e., ability to aid in predicting the target)"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#caveats-fa-skull",
    "href": "slides/3_feature-based/pfi.html#caveats-fa-skull",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Caveats ¬†¬†¬†",
    "text": "Caveats ¬†¬†¬†\n\nPFI is parameterised by:\n\ndata set\npredictive performance metric\nnumber of repetitions\n\nGenerating PFI may be computationally expensive for large sets of data and high number of repetitions\nComputational complexity: \\(\\mathcal{O} \\left( n \\times d \\right)\\), where\n\n\\(n\\) is the number of instances in the designated data set and\n\\(d\\) is the number of permutation repetitions"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#related-techniques",
    "href": "slides/3_feature-based/pfi.html#related-techniques",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Related Techniques",
    "text": "Related Techniques\n\nBuilt-in Feature Importance\n\nMany data-driven predictive models come equipped with some variant of feature importance. This includes  Decision Trees and  Linear Models among many others."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#related-techniques-meta-subs.ctd",
    "href": "slides/3_feature-based/pfi.html#related-techniques-meta-subs.ctd",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Related Techniques ¬†¬†¬†",
    "text": "Related Techniques ¬†¬†¬†\n\nPartial Dependence-based (PD) Feature Importance\n\n ¬†¬†¬† Partial Dependence captures the average response of a predictive model for a collection of instances when varying one of their features (Friedman 2001). By assessing flatness of these curves we can derive a feature importance measurement (Greenwell, Boehmke, and McCarthy 2018)."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#related-techniques-meta-subs.ctd-1",
    "href": "slides/3_feature-based/pfi.html#related-techniques-meta-subs.ctd-1",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Related Techniques ¬†¬†¬†",
    "text": "Related Techniques ¬†¬†¬†\n\nSHapley Additive exPlanations-based (SHAP) Feature Importance\n\n ¬†¬†¬† SHapley Additive exPlanations explains a prediction of a selected instance by using Shapley values to computing the contribution of each individual feature to this outcome (Lundberg and Lee 2017). It comes with various aggregation mechanisms that allow to transform individual explanations into global, model-based insights such as feature importance."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#related-techniques-meta-subs.ctd-2",
    "href": "slides/3_feature-based/pfi.html#related-techniques-meta-subs.ctd-2",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Related Techniques ¬†¬†¬†",
    "text": "Related Techniques ¬†¬†¬†\n\nLocal Interpretable Model-agnostic Explanations-based (LIME) Feature Importance\n\n ¬†¬†¬† Local Interpretable Model-agnostic Explanations is a surrogate explainer that fits a linear model to data (expressed in an interpretable representaion) sampled in the neighbourhood of an instance selected to be explained (Ribeiro, Singh, and Guestrin 2016). This local, inherently transparent model simplifies the black-box decision boundary in the selected sub-space, making it human-comprehensible. Given that these explanations are based on coefficients of the surrogate linear model, they can also be interpreted as (interpretable) feature importance."
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#implementations",
    "href": "slides/3_feature-based/pfi.html#implementations",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Implementations",
    "text": "Implementations\n\n\n\n\n\n\n\n Python\n R\n\n\n\n\nscikit-learn (>=0.24.0)\niml\n\n\nalibi\nvip\n\n\nSkater\nDALEX\n\n\nrfpimp"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#further-reading",
    "href": "slides/3_feature-based/pfi.html#further-reading",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Further Reading",
    "text": "Further Reading\n\nRandom Forests paper (Breiman 2001)\nModel Reliance paper (Fisher, Rudin, and Dominici 2019)\nOverview of feature importance techniques (Wei, Lu, and Song 2015)\nInterpretable Machine Learning book\nExplanatory Model Analysis book\nKaggle course\nscikit-learn examples: 1 & 2"
  },
  {
    "objectID": "slides/3_feature-based/pfi.html#bibliography",
    "href": "slides/3_feature-based/pfi.html#bibliography",
    "title": "Permutation Feature Importance (PFI)/Model Reliance/",
    "section": "Bibliography",
    "text": "Bibliography\n\n\nBreiman, Leo. 2001. ‚ÄúRandom Forests.‚Äù Machine Learning 45 (1): 5‚Äì32.\n\n\nFisher, Aaron, Cynthia Rudin, and Francesca Dominici. 2019. ‚ÄúAll Models Are Wrong, but Many Are Useful: Learning a Variable‚Äôs Importance by Studying an Entire Class of Prediction Models Simultaneously.‚Äù J. Mach. Learn. Res. 20 (177): 1‚Äì81.\n\n\nFriedman, Jerome H. 2001. ‚ÄúGreedy Function Approximation: A Gradient Boosting Machine.‚Äù Annals of Statistics, 1189‚Äì1232.\n\n\nGreenwell, Brandon M, Bradley C Boehmke, and Andrew J McCarthy. 2018. ‚ÄúA Simple and Effective Model-Based Variable Importance Measure.‚Äù arXiv Preprint arXiv:1805.04755.\n\n\nLundberg, Scott M, and Su-In Lee. 2017. ‚ÄúA Unified Approach to Interpreting Model Predictions.‚Äù Advances in Neural Information Processing Systems 30.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. ‚Äú‚ÄòWhy Should I Trust You?‚Äô Explaining the Predictions of Any Classifier.‚Äù In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135‚Äì44.\n\n\nWei, Pengfei, Zhenzhou Lu, and Jingwen Song. 2015. ‚ÄúVariable Importance Analysis: A Comprehensive Review.‚Äù Reliability Engineering & System Safety 142: 399‚Äì432."
  }
]